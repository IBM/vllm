"""Verification of vLLM output by comparing with HF

Run `python -m pytest tests/spyre/test_spyre_max_new_tokens.py`.
"""

from typing import List, Tuple

import pytest
from spyre_util import (compare_results, generate_hf_output,
                        generate_spyre_vllm_output)

from vllm import SamplingParams

template = (
    "Below is an instruction that describes a task. Write a response that "
    "appropriately completes the request. Be polite in your response to the "
    "user.\n\n### Instruction:\n{}\n\n### Response:")

prompt1 = template.format("Provide a recipe for chicken soup.")
prompt2 = template.format("Provide a list of instructions for preparing "
                          "chicken soup for a family of four.")

@pytest.mark.parametrize("model", ["/models/llama-194m"])
@pytest.mark.parametrize("prompts", [[prompt1, prompt2, prompt2, prompt2],
                                     [prompt2, prompt2, prompt2, prompt1],
                                     [prompt2, prompt2, prompt2, prompt2]])
@pytest.mark.parametrize("stop_last", [True, False])
@pytest.mark.parametrize("warmup_shape", [(64, 10, 4)]
                         )  # (prompt_length/new_tokens/batch_size)
@pytest.mark.parametrize("backend",
                         ["eager"])  #, "inductor", "sendnn_decoder"])
def test_output(
    model: str,
    prompts: List[str],
    stop_last: bool,
    warmup_shape: Tuple[int, int, int],
    backend: str,
) -> None:
    '''
    The warmup is based on a single shape. After the warmup,
    one request with the provided prompts is input to vLLM.
    The same prompts are also input to HF. The generated output
    including text, token ids, and logprobs, is verified to be
    identical for vLLM and HF.

    If errors occur, these can be analyzed/debugged by setting
    'DISABLE_ASSERTS = True' in spyre_util.py and by rerunning the
    test using 'pytest --capture=no tests/spyre/test_spyre_max_new_tokens.py'
    After debugging, DISABLE_ASSERTS should be reset to 'False'.
    '''
    
    max_new_tokens_warmup = warmup_shape[1]
    max_new_tokens_early_stop = 1

    vllm_sampling_params_normal = SamplingParams(
        max_tokens=max_new_tokens_warmup,
        temperature=0,
        logprobs=0,  # return logprobs of generated tokens only
        ignore_eos=False)
    
    vllm_sampling_params_early_stop = SamplingParams(
        max_tokens=max_new_tokens_early_stop,
        temperature=0,
        logprobs=0,  # return logprobs of generated tokens only
        ignore_eos=False)
    
    vllm_sampling_params = [vllm_sampling_params_normal] * 3
    max_new_tokens = [max_new_tokens_warmup] * 3

    # stop last or first sequence in batch early
    if stop_last:
        vllm_sampling_params = vllm_sampling_params + [vllm_sampling_params_early_stop]
        max_new_tokens = max_new_tokens + [max_new_tokens_early_stop]
    else: 
        vllm_sampling_params = [vllm_sampling_params_early_stop] + vllm_sampling_params 
        max_new_tokens = [max_new_tokens_early_stop] + max_new_tokens 

    vllm_results = generate_spyre_vllm_output(
        model=model,
        prompts=prompts,
        warmup_shapes=[warmup_shape],
        max_model_len=2048,
        block_size=2048,
        sampling_params=vllm_sampling_params,
        tensor_parallel_size=1,
        backend=backend)

    hf_results = generate_hf_output(model=model,
                                    prompts=prompts,
                                    max_new_tokens=max_new_tokens)

    compare_results(model=model,
                    prompts=prompts,
                    warmup_shapes=[warmup_shape],
                    tensor_parallel_size=1,
                    backend=backend,
                    vllm_results=vllm_results,
                    hf_results=hf_results)
