{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "786d5912",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "%load_ext wurlitzer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f873ca1",
   "metadata": {},
   "source": [
    "Online inference demo\n",
    "----------------------------\n",
    "This is just a brief demo to show that vLLM with Spyre can be used in the online mode. \n",
    "\n",
    "Hence, a vLLM server must be started before (outside of this notebook):\n",
    "```bash\n",
    "python3 -m vllm.entrypoints.openai.api_server --model /models/llama-7b-chat --max-model-len=2048 --block-size=2048\n",
    "```\n",
    "\n",
    "and waited until vLLM is ready, which is after the following log messages were printed (otherwise, there will be `ConnectError`s in the code below):\n",
    "```log\n",
    "INFO:     Started server process [1840]\n",
    "INFO:     Waiting for application startup.\n",
    "INFO:     Application startup complete.\n",
    "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
    "```\n",
    "\n",
    "(The startup of vLLM, including warmup of Spyre, is expected to take 15 min.)\n",
    "\n",
    "Here, the default max prompt length of 64 and maximum of 20 decode tokens is used. Otherwise change this behavior with the environment variables `VLLM_SPYRE_WARMUP_PROMPT_LENS`, and `VLLM_SPYRE_WARMUP_NEW_TOKENS`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d4924c",
   "metadata": {},
   "source": [
    "### 1. Create the prompts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb328e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = (\n",
    "    \"Below is an instruction that describes a task. Write a response that \"\n",
    "    \"appropriately completes the request. Be polite in your response to the \"\n",
    "    \"user.\\n\\n### Instruction:\\n{}\\n\\n### Response:\"\n",
    ")\n",
    "prompt1 = template.format(\n",
    "    \"Provide a list of instructions for preparing chicken soup for a family \"\n",
    "    \"of four.\"\n",
    ")\n",
    "\n",
    "prompt2 = template.format(\n",
    "    \"Please compare New York City and Zurich and provide a list of attractions \"\n",
    "    \"for each city.\"\n",
    ")\n",
    "\n",
    "prompt3 = template.format(\n",
    "    \"Provide detailed instructions for preparing asparagus soup for a family \"\n",
    "    \"of four.\"\n",
    ")\n",
    "\n",
    "prompts = [prompt1, prompt2, prompt3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d0cee55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Below is an instruction that describes a task. Write a response that appropriately completes the request. Be polite in your response to the user.\\n\\n### Instruction:\\nPlease compare New York City and Zurich and provide a list of attractions for each city.\\n\\n### Response:'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# e.g. prompt 2\n",
    "prompt2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c92859",
   "metadata": {},
   "source": [
    "### 2. Initialize client and connect to vLLM\n",
    "\n",
    "(Adapt the `openai_api_base` URL to point to the (forwarded/tunneled) vLLM instance. E.g. forward it to localhost with `oc port-forward $DEV_POD 8000`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7dee2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "models = client.models.list()\n",
    "model = models.data[0].id\n",
    "\n",
    "# Completion API\n",
    "stream = False\n",
    "max_tokens = 20  # default\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026153e2",
   "metadata": {},
   "source": [
    "### 3. Submit requests and await responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bceb2c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Below is an instruction that describes a task. Write a response that appropriately completes the request. Be polite in your response to the user.\n",
      "\n",
      "### Instruction:\n",
      "Provide a list of instructions for preparing chicken soup for a family of four.\n",
      "\n",
      "### Response:\n",
      "Results:\n",
      "[CompletionChoice(finish_reason='length', index=0, logprobs=None, text='\\nOf course! Here are the steps to prepare chicken soup for a family of four:\\n', stop_reason=None)]\n",
      "Duration: 3.3749101161956787s\n",
      "---------------------------\n",
      "\n",
      "Prompt: Below is an instruction that describes a task. Write a response that appropriately completes the request. Be polite in your response to the user.\n",
      "\n",
      "### Instruction:\n",
      "Please compare New York City and Zurich and provide a list of attractions for each city.\n",
      "\n",
      "### Response:\n",
      "Results:\n",
      "[CompletionChoice(finish_reason='length', index=0, logprobs=None, text='\\nThank you for reaching out! Both New York City and Zurich are incredible destinations with', stop_reason=None)]\n",
      "Duration: 3.367875576019287s\n",
      "---------------------------\n",
      "\n",
      "Prompt: Below is an instruction that describes a task. Write a response that appropriately completes the request. Be polite in your response to the user.\n",
      "\n",
      "### Instruction:\n",
      "Provide detailed instructions for preparing asparagus soup for a family of four.\n",
      "\n",
      "### Response:\n",
      "Results:\n",
      "[CompletionChoice(finish_reason='length', index=0, logprobs=None, text='\\nOf course! Preparing asparagus soup for a family of four is a straightforward', stop_reason=None)]\n",
      "Duration: 3.3706459999084473s\n",
      "---------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    start_t = time.time()\n",
    "\n",
    "    completion = client.completions.create(\n",
    "        model=model,\n",
    "        prompt=prompt,\n",
    "        echo=False,\n",
    "        n=1,\n",
    "        stream=stream,\n",
    "        temperature=0.0,\n",
    "        max_tokens=max_tokens)\n",
    "\n",
    "    end_t = time.time()\n",
    "    print(\"Results:\")\n",
    "    if stream:\n",
    "        for c in completion:\n",
    "            print(c)\n",
    "    else:\n",
    "        print(completion.choices)\n",
    "\n",
    "    total_t = end_t - start_t\n",
    "    print(f\"Duration: {total_t}s\")\n",
    "    print(\"---------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1e686d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
