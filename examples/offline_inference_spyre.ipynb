{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb1996e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-15 14:54:53 _custom_ops.py:14] Failed to import from vllm._C with ModuleNotFoundError(\"No module named 'vllm._C'\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/vllm/lib64/python3.9/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
      "No module named 'vllm.commit_id'\n",
      "  from vllm.version import __version__ as VLLM_VERSION\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "%load_ext wurlitzer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45172614",
   "metadata": {},
   "source": [
    "Offline inference demo\n",
    "----------------------------\n",
    "This is just a brief demo to show that vLLM with Spyre can be used in the offline mode. \n",
    "\n",
    "vLLM will determine the Spyre config automatically and warmup the Spyre stack. \n",
    "The startup of vLLM (including warmup of Spyre), is expected to take 15 min for prompt length of 64 and maximum number of decode tokens 5 (it will take ~20min for 64/20)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf37c07b",
   "metadata": {},
   "source": [
    "### 1. create vLLM instance \n",
    "(for offline usage, including warmup)\n",
    "\n",
    "The maximum prompt length and maximum number of decode tokens can be specified using the environment variables `VLLM_SPYRE_WARMUP_PROMPT_LENS`, and `VLLM_SPYRE_WARMUP_NEW_TOKENS`. \n",
    "Otherwise the default max prompt length of 64 and maximum of 20 decode tokens will be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecf0992b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['VLLM_SPYRE_WARMUP_PROMPT_LENS'] = '64'\n",
    "os.environ['VLLM_SPYRE_WARMUP_NEW_TOKENS'] = '5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88b984d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-15 14:54:54 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='/tmp/7B-F', speculative_config=None, tokenizer='/tmp/7B-F', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cpu, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=/tmp/7B-F, use_v2_block_manager=False, enable_prefix_caching=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Disabled: dynamo_tracer\n",
      "WARNING 08-15 14:54:55 utils.py:581] Pin memory is not supported on Spyre device.\n",
      "[SpyreWorker] environment configured\n",
      "[SpyreWorker] load model...\n",
      ">> DEBUG  SETUP\n",
      "0 / 1 : Python Version : 3.9.18\n",
      "0 / 1 : PyTorch Version: 2.2.2+cpu\n",
      "0 / 1 : PCI Addr Rank 0 AIU_WORLD_RANK_0=0\n",
      "0 / 1 : PCI Addr Rank 0 FLEX_RDMA_PCI_BUS_ADDR_0=0000:1e:00.0\n",
      "0 / 1 : FLEX_COMPUTE=SENTIENT\n",
      "0 / 1 : FLEX_DEVICE=VFIO\n",
      "0 / 1 : DEEPRT_EXPORT_DIR=export/0\n",
      "0 / 1 : DTCOMPILER_EXPORT_DIR=export/0\n",
      "0 / 1 : AIU_CONFIG_FILE_0=/etc/aiu/senlib_config.json\n",
      "0 / 1 : SENLIB_DEVEL_CONFIG_FILE=/etc/aiu/senlib_config.json\n",
      "0 / 1 : FLEX_RDMA_PCI_BUS_ADDR_0=0000:1e:00.0\n",
      "0 / 1 : FLEX_RDMA_LOCAL_RANK=0\n",
      "0 / 1 : FLEX_RDMA_LOCAL_SIZE=1\n",
      "0 / 1 : FLEX_RDMA_WORLD_RANK=0\n",
      "0 / 1 : FLEX_RDMA_WORLD_SIZE=1\n",
      "0 / 1 : Spyre: Enabled (0) (offset=0)\n",
      "0 / 1 : Dynamo Backend  : sendnn_decoder\n",
      "0 / 1 : CPU Cores       : 56 x 2 HW threads\n",
      "------------------------------------------------------------\n",
      "NOTICE: Adjusting torch._dynamo.config.accumulated_cache_size_limit from 64 to 160 to accommodate prompt size of 64 and decode tokens of 5\n",
      "NOTICE: Adjusting torch._dynamo.config.cache_size_limit from 8 to 160 to accommodate prompt size of 64 and decode tokens of 5\n",
      "\tload model took 62.92104411125183s\n",
      "[SpyreWorker] Start warming up 1 different prompt/decode-shape combinations.\n",
      "[SpyreWorker] Warmup 1/1 prompt/decode-shape combinations...\n",
      "[SpyreWorker] warmup for prompt length 64 and max output tokens 5.\n",
      "[SpyreWorker] warmup 1/2...\n",
      "[SpyreWorker] warmup 2/2...\n",
      "update_lazyhandle() done (duration: 134.3403525352478s)\n",
      "[SpyreWorker] ... warmup finished.\n",
      "\twarmup took 893.4236354827881s (for prompt length 64 and max output tokens 5)\n",
      "[SpyreWorker] All warmups for 1 different prompt/decode-shape combinations finished. Total warmup time 893.4242045879364s.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Create an LLM.\n",
    "llm = LLM(\n",
    "    model=\"/models/llama-7b-chat\",\n",
    "    tokenizer=\"/models/llama-7b-chat\",\n",
    "    max_model_len=2048,\n",
    "    block_size=2048,\n",
    "    device=\"spyre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818488f4",
   "metadata": {},
   "source": [
    "### 2. Create the prompt  and `SamplingParams`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c32e3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Below is an instruction that describes a task. Write a response that appropriately completes the request. Be polite in your response to the user.\\n\\n### Instruction:\\nProvide a list of instructions for preparing chicken soup for a family of four.\\n\\n### Response:']\n"
     ]
    }
   ],
   "source": [
    "# Sample prompts.\n",
    "template = (\n",
    "    \"Below is an instruction that describes a task. Write a response that \"\n",
    "    \"appropriately completes the request. Be polite in your response to the \"\n",
    "    \"user.\\n\\n### Instruction:\\n{}\\n\\n### Response:\"\n",
    ")\n",
    "prompt1 = template.format(\n",
    "    \"Provide a list of instructions for preparing chicken soup for a family \"\n",
    "    \"of four.\"\n",
    ")\n",
    "prompts = [\n",
    "    prompt1,\n",
    "]\n",
    "print(prompts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cc1277e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sampling params object.\n",
    "max_tokens = 5\n",
    "sampling_params = SamplingParams(max_tokens=max_tokens, temperature=0.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffb16c5",
   "metadata": {},
   "source": [
    "### 3. Generate the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "522c0610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============== GENERATE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[spyre_model_runner:execute_model] t_token: 195.92ms\n",
      "[spyre_model_runner:execute_model] t_token: 158.88ms\n",
      "[spyre_model_runner:execute_model] t_token: 158.49ms\n",
      "[spyre_model_runner:execute_model] t_token: 158.88ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.20it/s, est. speed input: 76.65 toks/s, output: 5.99 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[spyre_model_runner:execute_model] t_token: 158.67ms\n",
      "Time elaspsed for 5 tokens is 0.84 sec\n",
      "Prompt: 'Below is an instruction that describes a task. Write a response that appropriately completes the request. Be polite in your response to the user.\\n\\n### Instruction:\\nProvide a list of instructions for preparing chicken soup for a family of four.\\n\\n### Response:', Generated text: '\\nOf course! Here'\n",
      "CompletionOutput(index=0, text='\\nOf course! Here', token_ids=(13, 2776, 3236, 29991, 2266), cumulative_logprob=-0.9147708129385137, logprobs=None, finish_reason=length, stop_reason=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=============== GENERATE\")\n",
    "t0 = time.time()\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "print(\"Time elaspsed for %d tokens is %.2f sec\" % \n",
    "      (len(outputs[0].outputs[0].token_ids), time.time()-t0))\n",
    "for output in outputs:\n",
    "   prompt = output.prompt\n",
    "   generated_text = output.outputs[0].text\n",
    "   print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n",
    "print(output.outputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221b2d2b",
   "metadata": {},
   "source": [
    "Expectation (for the 2nd+ tokens): \n",
    "- ~158ms per token if the model was warmed up with max 5 output tokens\n",
    "- ~162ms per token if the model was warmed up with max 20 output tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd23c547",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
