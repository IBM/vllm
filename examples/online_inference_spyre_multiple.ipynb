{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cae04a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "%load_ext wurlitzer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f873ca1",
   "metadata": {},
   "source": [
    "Online inference demo with multiple prompt lengths\n",
    "------------------------------------------------------\n",
    "This is just a brief demo to show that vLLM with Spyre can be used in the online mode with MULTIPLE prompt-length/max-decode shapes!  \n",
    "\n",
    "Hence, a vLLM server must be started before (outside of this notebook). We assume the following:\n",
    "```bash\n",
    "export VLLM_SPYRE_WARMUP_PROMPT_LENS=64,128\n",
    "export VLLM_SPYRE_WARMUP_NEW_TOKENS=20,10\n",
    "export VLLM_SPYRE_WARMUP_BATCH_SIZES=1,1\n",
    "python3 -m vllm.entrypoints.openai.api_server --model /models/llama-7b-chat --max-model-len=2048 --block-size=2048\n",
    "```\n",
    "\n",
    "Then, we need to wait until vLLM is ready, which is after the following log messages were printed (otherwise, there will be `ConnectError`s in the code below):\n",
    "```log\n",
    "INFO:     Started server process [1840]\n",
    "INFO:     Waiting for application startup.\n",
    "INFO:     Application startup complete.\n",
    "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
    "```\n",
    "\n",
    "(The startup of vLLM, including warmup of Spyre for both shapes, is expected to take ~35 min.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d4924c",
   "metadata": {},
   "source": [
    "### 1. Create the prompts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb328e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = (\n",
    "    \"Below is an instruction that describes a task. Write a response that \"\n",
    "    \"appropriately completes the request. Be polite in your response to the \"\n",
    "    \"user.\\n\\n### Instruction:\\n{}\\n\\n### Response:\"\n",
    ")\n",
    "\n",
    "prompt1 = template.format(\n",
    "    \"Provide a list of instructions for preparing chicken soup for a family \"\n",
    "    \"of four.\"\n",
    ")\n",
    "\n",
    "prompt2 = template.format(\n",
    "    \"Please compare the Cities of New York and Zurich and provide a list of \"\n",
    "    \"attractions for each city to visit in one day.\"\n",
    ")\n",
    "\n",
    "prompt3 = template.format(\n",
    "    \"Provide detailed instructions for preparing asparagus soup for a family \"\n",
    "    \"of four using lots of cream.\"\n",
    ")\n",
    "\n",
    "prompts = [prompt1, prompt2, prompt3]\n",
    "max_tokens_list = [20, 10, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee30282f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Below is an instruction that describes a task. Write a response that appropriately completes the request. Be polite in your response to the user.\\n\\n### Instruction:\\nPlease compare the Cities of New York and Zurich and provide a list of attractions for each city to visit in one day.\\n\\n### Response:'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# e.g. prompt 2\n",
    "prompt2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c92859",
   "metadata": {},
   "source": [
    "### 2. Initialize client and connect to vLLM\n",
    "\n",
    "(Adapt the `openai_api_base` URL to point to the (forwarded/tunneled) vLLM instance. E.g. forward it to localhost with `oc port-forward $DEV_POD 8000`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7dee2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "models = client.models.list()\n",
    "model = models.data[0].id\n",
    "\n",
    "# Completion API\n",
    "stream = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026153e2",
   "metadata": {},
   "source": [
    "### 3. Submit requests and await responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bceb2c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Below is an instruction that describes a task. Write a response that appropriately completes the request. Be polite in your response to the user.\n",
      "\n",
      "### Instruction:\n",
      "Provide a list of instructions for preparing chicken soup for a family of four.\n",
      "\n",
      "### Response:\n",
      "Results:\n",
      "\n",
      "Of course! Here are the steps to prepare chicken soup for a family of four:\n",
      "\n",
      "Duration: 3.6192898750305176s\n",
      "---------------------------\n",
      "\n",
      "Prompt: Below is an instruction that describes a task. Write a response that appropriately completes the request. Be polite in your response to the user.\n",
      "\n",
      "### Instruction:\n",
      "Please compare the Cities of New York and Zurich and provide a list of attractions for each city to visit in one day.\n",
      "\n",
      "### Response:\n",
      "Results:\n",
      "\n",
      "Thank you for reaching out! Both New York\n",
      "Duration: 1.6599247455596924s\n",
      "---------------------------\n",
      "\n",
      "Prompt: Below is an instruction that describes a task. Write a response that appropriately completes the request. Be polite in your response to the user.\n",
      "\n",
      "### Instruction:\n",
      "Provide detailed instructions for preparing asparagus soup for a family of four using lots of cream.\n",
      "\n",
      "### Response:\n",
      "Results:\n",
      "\n",
      "Of course! Here are the steps to prepare\n",
      "Duration: 1.6462435722351074s\n",
      "---------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for prompt, max_tokens in zip(prompts, max_tokens_list):\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    start_t = time.time()\n",
    "\n",
    "    completion = client.completions.create(\n",
    "        model=model,\n",
    "        prompt=prompt,\n",
    "        echo=False,\n",
    "        n=1,\n",
    "        stream=stream,\n",
    "        temperature=0.0,\n",
    "        max_tokens=max_tokens)\n",
    "\n",
    "    end_t = time.time()\n",
    "    print(\"Results:\")\n",
    "    if stream:\n",
    "        for c in completion:\n",
    "            print(c)\n",
    "    else:\n",
    "        # print(completion.choices)\n",
    "        print(completion.choices[0].text)\n",
    "\n",
    "    total_t = end_t - start_t\n",
    "    print(f\"Duration: {total_t}s\")\n",
    "    print(\"---------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1e686d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
