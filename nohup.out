INFO 05-28 20:19:14 [__init__.py:248] Automatically detected platform cuda.
INFO 05-28 20:19:17 [api_server.py:1042] vLLM API server version 0.1.dev6336+gbe40f32
INFO 05-28 20:19:17 [api_server.py:1043] args: Namespace(subparser='serve', model_tag='ibm-granite/granite-3.2-8b-instruct', config='', host=None, port=8000, uvicorn_log_level='info', disable_uvicorn_access_log=False, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=[LoRAModulePath(name='new_alora', path='/proj/dmfexp/statllm/users/kgreenewald/.cache/huggingface/models/hub/models--ibm-granite--granite-3.2-8b-alora-uncertainty/snapshots/6109ad88201426003e696d023ec67c19e7f3d444', base_model_name='ibm-granite/granite-3.2-8b-instruct')], prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='ibm-granite/granite-3.2-8b-instruct', task='auto', tokenizer=None, tokenizer_mode='auto', trust_remote_code=False, dtype='bfloat16', seed=None, hf_config_path=None, allowed_local_media_path='', revision=None, code_revision=None, rope_scaling={}, rope_theta=None, tokenizer_revision=None, max_model_len=None, quantization=None, enforce_eager=True, max_seq_len_to_capture=8192, max_logprobs=20, disable_sliding_window=False, disable_cascade_attn=False, skip_tokenizer_init=False, enable_prompt_embeds=False, served_model_name=None, disable_async_output_proc=False, config_format='auto', hf_token=None, hf_overrides={}, override_neuron_config={}, override_pooler_config=None, logits_processor_pattern=None, generation_config='auto', override_generation_config={}, enable_sleep_mode=False, model_impl='auto', load_format='auto', download_dir=None, model_loader_extra_config={}, ignore_patterns=None, use_tqdm_on_load=True, qlora_adapter_name_or_path=None, pt_load_map_location='cpu', guided_decoding_backend='auto', guided_decoding_disable_fallback=False, guided_decoding_disable_any_whitespace=False, guided_decoding_disable_additional_properties=False, enable_reasoning=None, reasoning_parser='', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, data_parallel_size=1, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, disable_custom_all_reduce=False, worker_cls='auto', worker_extension_cls='', block_size=None, gpu_memory_utilization=0.9, swap_space=4, kv_cache_dtype='auto', num_gpu_blocks_override=None, enable_prefix_caching=False, prefix_caching_hash_algo='builtin', cpu_offload_gb=0, calculate_kv_scales=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config={}, limit_mm_per_prompt={}, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=True, enable_lora_bias=False, max_loras=1, max_lora_rank=64, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=None, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', speculative_config=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, max_num_batched_tokens=None, max_num_seqs=None, max_num_partial_prefills=1, max_long_partial_prefills=1, cuda_graph_sizes=[512], long_prefill_token_threshold=0, num_lookahead_slots=0, scheduler_delay_factor=0.0, preemption_mode=None, num_scheduler_steps=1, multi_step_stream_outputs=True, scheduling_policy='fcfs', enable_chunked_prefill=None, disable_chunked_mm_input=False, scheduler_cls='vllm.core.scheduler.Scheduler', compilation_config=None, kv_transfer_config=None, kv_events_config=None, additional_config=None, use_v2_block_manager=True, disable_log_stats=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False, dispatch_function=<function ServeSubcommand.cmd at 0x150b7a614400>)
INFO 05-28 20:19:25 [config.py:752] This model supports multiple tasks: {'classify', 'generate', 'embed', 'reward', 'score'}. Defaulting to 'generate'.
INFO 05-28 20:19:25 [config.py:2057] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 05-28 20:19:25 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 05-28 20:19:28 [__init__.py:248] Automatically detected platform cuda.
INFO 05-28 20:19:30 [core.py:59] Initializing a V1 LLM engine (v0.1.dev6336+gbe40f32) with config: model='ibm-granite/granite-3.2-8b-instruct', speculative_config=None, tokenizer='ibm-granite/granite-3.2-8b-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=ibm-granite/granite-3.2-8b-instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=False, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}
WARNING 05-28 20:19:30 [utils.py:2621] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x146567d4c4a0>
INFO 05-28 20:19:31 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 05-28 20:19:31 [cuda.py:221] Using Flash Attention backend on V1 engine.
WARNING 05-28 20:19:31 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
INFO 05-28 20:19:31 [gpu_model_runner.py:1436] Starting to load model ibm-granite/granite-3.2-8b-instruct...
INFO 05-28 20:19:32 [weight_utils.py:257] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:05,  1.77s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:03<00:03,  1.80s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:04<00:01,  1.23s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:05<00:00,  1.45s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:05<00:00,  1.48s/it]

INFO 05-28 20:19:38 [default_loader.py:278] Loading weights took 6.01 seconds
INFO 05-28 20:19:38 [punica_selector.py:18] Using PunicaWrapperGPU.
INFO 05-28 20:19:38 [gpu_model_runner.py:1454] Model loading took 15.6392 GiB and 6.427371 seconds
INFO 05-28 20:19:39 [kv_cache_utils.py:643] GPU KV cache size: 346,592 tokens
INFO 05-28 20:19:39 [kv_cache_utils.py:646] Maximum concurrency for 131,072 tokens per request: 2.64x
INFO 05-28 20:19:39 [core.py:161] init engine (profile, create kv cache, warmup model) took 1.17 seconds
INFO 05-28 20:19:39 [core_client.py:442] Core engine process 0 ready.
INFO 05-28 20:19:39 [loggers.py:136] vllm cache_config_info with initialization after num_gpu_blocks is: 21662
INFO 05-28 20:19:39 [serving_models.py:185] Loaded new LoRA adapter: name 'new_alora', path '/proj/dmfexp/statllm/users/kgreenewald/.cache/huggingface/models/hub/models--ibm-granite--granite-3.2-8b-alora-uncertainty/snapshots/6109ad88201426003e696d023ec67c19e7f3d444'
INFO 05-28 20:19:39 [api_server.py:1089] Starting vLLM API server on http://0.0.0.0:8000
INFO 05-28 20:19:39 [launcher.py:28] Available routes are:
INFO 05-28 20:19:39 [launcher.py:36] Route: /openapi.json, Methods: HEAD, GET
INFO 05-28 20:19:39 [launcher.py:36] Route: /docs, Methods: HEAD, GET
INFO 05-28 20:19:39 [launcher.py:36] Route: /docs/oauth2-redirect, Methods: HEAD, GET
INFO 05-28 20:19:39 [launcher.py:36] Route: /redoc, Methods: HEAD, GET
INFO 05-28 20:19:39 [launcher.py:36] Route: /health, Methods: GET
INFO 05-28 20:19:39 [launcher.py:36] Route: /load, Methods: GET
INFO 05-28 20:19:39 [launcher.py:36] Route: /ping, Methods: POST, GET
INFO 05-28 20:19:39 [launcher.py:36] Route: /tokenize, Methods: POST
INFO 05-28 20:19:39 [launcher.py:36] Route: /detokenize, Methods: POST
INFO 05-28 20:19:39 [launcher.py:36] Route: /v1/models, Methods: GET
INFO 05-28 20:19:39 [launcher.py:36] Route: /version, Methods: GET
INFO 05-28 20:19:39 [launcher.py:36] Route: /v1/chat/completions, Methods: POST
INFO 05-28 20:19:39 [launcher.py:36] Route: /v1/completions, Methods: POST
INFO 05-28 20:19:39 [launcher.py:36] Route: /v1/embeddings, Methods: POST
INFO 05-28 20:19:39 [launcher.py:36] Route: /pooling, Methods: POST
INFO 05-28 20:19:39 [launcher.py:36] Route: /score, Methods: POST
INFO 05-28 20:19:39 [launcher.py:36] Route: /v1/score, Methods: POST
INFO 05-28 20:19:39 [launcher.py:36] Route: /v1/audio/transcriptions, Methods: POST
INFO 05-28 20:19:39 [launcher.py:36] Route: /rerank, Methods: POST
INFO 05-28 20:19:39 [launcher.py:36] Route: /v1/rerank, Methods: POST
INFO 05-28 20:19:39 [launcher.py:36] Route: /v2/rerank, Methods: POST
INFO 05-28 20:19:39 [launcher.py:36] Route: /invocations, Methods: POST
INFO 05-28 20:19:39 [launcher.py:36] Route: /metrics, Methods: GET
INFO:     Started server process [2501521]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO 05-28 20:20:00 [logger.py:39] Received request cmpl-bcf23640092e4eafb19ff348857f46c2-0: prompt: 'What is MIT?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=600, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [8197, 438, 7131, 49], lora_request: None, prompt_adapter_request: None.
INFO 05-28 20:20:00 [logger.py:39] Received request cmpl-bcf23640092e4eafb19ff348857f46c2-1: prompt: '<|start_of_role|>user<|end_of_role|>What is MIT?<|end_of_text|>\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=600, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [49152, 496, 49153, 8197, 438, 7131, 49, 0, 203], lora_request: None, prompt_adapter_request: None.
INFO 05-28 20:20:00 [async_llm.py:255] Added request cmpl-bcf23640092e4eafb19ff348857f46c2-0.
INFO 05-28 20:20:00 [async_llm.py:255] Added request cmpl-bcf23640092e4eafb19ff348857f46c2-1.
INFO 05-28 20:20:10 [loggers.py:116] Engine 000: Avg prompt throughput: 1.3 tokens/s, Avg generation throughput: 74.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
INFO:     127.0.0.1:44874 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-28 20:20:20 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 05-28 20:20:30 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 05-28 20:22:32 [logger.py:39] Received request cmpl-db45b7ba85664ea4a4c6df848b1596f9-0: prompt: 'What is MIT?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=600, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [8197, 438, 7131, 49], lora_request: None, prompt_adapter_request: None.
INFO 05-28 20:22:32 [logger.py:39] Received request cmpl-db45b7ba85664ea4a4c6df848b1596f9-1: prompt: '<|start_of_role|>user<|end_of_role|>What is MIT?<|end_of_text|>\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=600, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [49152, 496, 49153, 8197, 438, 7131, 49, 0, 203], lora_request: None, prompt_adapter_request: None.
INFO 05-28 20:22:32 [async_llm.py:255] Added request cmpl-db45b7ba85664ea4a4c6df848b1596f9-0.
INFO 05-28 20:22:32 [async_llm.py:255] Added request cmpl-db45b7ba85664ea4a4c6df848b1596f9-1.
INFO 05-28 20:22:40 [loggers.py:116] Engine 000: Avg prompt throughput: 1.3 tokens/s, Avg generation throughput: 63.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
INFO:     127.0.0.1:57962 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-28 20:22:50 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 12.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 05-28 20:23:00 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 05-28 20:23:42 [logger.py:39] Received request cmpl-f71813501dba4d00bc7f2df963211e2c-0: prompt: 'What is MIT?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=600, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [8197, 438, 7131, 49], lora_request: None, prompt_adapter_request: None.
INFO 05-28 20:23:42 [logger.py:39] Received request cmpl-f71813501dba4d00bc7f2df963211e2c-1: prompt: '<|start_of_role|>user<|end_of_role|>What is MIT?<|end_of_text|>\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=600, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [49152, 496, 49153, 8197, 438, 7131, 49, 0, 203], lora_request: None, prompt_adapter_request: None.
INFO 05-28 20:23:42 [async_llm.py:255] Added request cmpl-f71813501dba4d00bc7f2df963211e2c-0.
INFO 05-28 20:23:42 [async_llm.py:255] Added request cmpl-f71813501dba4d00bc7f2df963211e2c-1.
INFO 05-28 20:23:50 [loggers.py:116] Engine 000: Avg prompt throughput: 1.3 tokens/s, Avg generation throughput: 65.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
INFO:     127.0.0.1:47212 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-28 20:24:00 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 10.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 05-28 20:24:10 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 05-28 20:24:17 [logger.py:39] Received request cmpl-7edf1ae3bae148a397b302a1a8c4c25e-0: prompt: 'What is MIT?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=600, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [8197, 438, 7131, 49], lora_request: None, prompt_adapter_request: None.
INFO 05-28 20:24:17 [logger.py:39] Received request cmpl-7edf1ae3bae148a397b302a1a8c4c25e-1: prompt: '<|start_of_role|>user<|end_of_role|>What is MIT?<|end_of_text|>\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=600, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [49152, 496, 49153, 8197, 438, 7131, 49, 0, 203], lora_request: None, prompt_adapter_request: None.
INFO 05-28 20:24:17 [async_llm.py:255] Added request cmpl-7edf1ae3bae148a397b302a1a8c4c25e-0.
INFO 05-28 20:24:17 [async_llm.py:255] Added request cmpl-7edf1ae3bae148a397b302a1a8c4c25e-1.
INFO 05-28 20:24:20 [loggers.py:116] Engine 000: Avg prompt throughput: 1.3 tokens/s, Avg generation throughput: 24.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
INFO:     127.0.0.1:36176 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-28 20:24:30 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 51.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 05-28 20:24:40 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 05-28 20:26:15 [logger.py:39] Received request cmpl-038fd7348ab048f6a7bed73da571714b-0: prompt: 'What is MIT?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=600, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [8197, 438, 7131, 49], lora_request: None, prompt_adapter_request: None.
INFO 05-28 20:26:15 [logger.py:39] Received request cmpl-038fd7348ab048f6a7bed73da571714b-1: prompt: '<|start_of_role|>user<|end_of_role|>What is MIT?<|end_of_text|>\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=600, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [49152, 496, 49153, 8197, 438, 7131, 49, 0, 203], lora_request: None, prompt_adapter_request: None.
INFO 05-28 20:26:15 [async_llm.py:255] Added request cmpl-038fd7348ab048f6a7bed73da571714b-0.
INFO 05-28 20:26:15 [async_llm.py:255] Added request cmpl-038fd7348ab048f6a7bed73da571714b-1.
INFO 05-28 20:26:20 [loggers.py:116] Engine 000: Avg prompt throughput: 1.3 tokens/s, Avg generation throughput: 42.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
INFO:     127.0.0.1:53858 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-28 20:26:30 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 33.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 05-28 20:26:40 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 05-28 20:29:23 [logger.py:39] Received request cmpl-efca7de8e2ac4c2a8c2a4f64e4203ccd-0: prompt: 'What is MIT?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=600, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [8197, 438, 7131, 49], lora_request: None, prompt_adapter_request: None.
INFO 05-28 20:29:23 [logger.py:39] Received request cmpl-efca7de8e2ac4c2a8c2a4f64e4203ccd-1: prompt: '<|start_of_role|>user<|end_of_role|>What is MIT?<|end_of_text|>\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=600, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [49152, 496, 49153, 8197, 438, 7131, 49, 0, 203], lora_request: None, prompt_adapter_request: None.
INFO 05-28 20:29:23 [async_llm.py:255] Added request cmpl-efca7de8e2ac4c2a8c2a4f64e4203ccd-0.
INFO 05-28 20:29:23 [async_llm.py:255] Added request cmpl-efca7de8e2ac4c2a8c2a4f64e4203ccd-1.
INFO 05-28 20:29:30 [loggers.py:116] Engine 000: Avg prompt throughput: 1.3 tokens/s, Avg generation throughput: 58.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
INFO:     127.0.0.1:56436 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-28 20:29:40 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 05-28 20:29:50 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 05-28 20:30:35 [logger.py:39] Received request cmpl-d54aa7dc852a46219d6117166027ed3c-0: prompt: 'What is MIT?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=600, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [8197, 438, 7131, 49], lora_request: None, prompt_adapter_request: None.
INFO 05-28 20:30:35 [logger.py:39] Received request cmpl-d54aa7dc852a46219d6117166027ed3c-1: prompt: '<|start_of_role|>user<|end_of_role|>What is MIT?<|end_of_text|>\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=600, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [49152, 496, 49153, 8197, 438, 7131, 49, 0, 203], lora_request: None, prompt_adapter_request: None.
INFO 05-28 20:30:35 [async_llm.py:255] Added request cmpl-d54aa7dc852a46219d6117166027ed3c-0.
INFO 05-28 20:30:35 [async_llm.py:255] Added request cmpl-d54aa7dc852a46219d6117166027ed3c-1.
INFO 05-28 20:30:40 [loggers.py:116] Engine 000: Avg prompt throughput: 1.3 tokens/s, Avg generation throughput: 41.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
INFO:     127.0.0.1:48236 - "POST /v1/completions HTTP/1.1" 200 OK
WARNING 05-28 20:30:45 [tokenizer.py:294] No tokenizer found in /proj/dmfexp/statllm/users/kgreenewald/.cache/huggingface/models/hub/models--ibm-granite--granite-3.2-8b-alora-uncertainty/snapshots/6109ad88201426003e696d023ec67c19e7f3d444, using base model tokenizer instead. (Exception: <class 'transformers.models.granite.configuration_granite.GraniteConfig'>)
INFO 05-28 20:30:45 [logger.py:39] Received request cmpl-85bd61a9372e4a71b367e4c47e5d5cf2-0: prompt: "What is MIT?\n\nMIT is the Massachusetts Institute of Technology, a private research university in Cambridge, Massachusetts. It is known for its strong programs in science, engineering, and technology, as well as its emphasis on innovation and entrepreneurship.\n\nWhat is the MIT Media Lab?\n\nThe MIT Media Lab is a research laboratory within MIT that focuses on the intersection of technology, art, design, and science. It was founded in 1985 and is known for its interdisciplinary approach to research and its emphasis on collaboration and innovation.\n\nWhat is the MIT Media Lab's mission?\n\nThe MIT Media Lab's mission is to explore the confluence of art, design, science, and technology. It aims to create new media and technologies that can be used to address complex problems and improve the human condition.\n\nWhat is the MIT Media Lab's approach to research?\n\nThe MIT Media Lab takes an interdisciplinary approach to research, bringing together experts from a wide range of fields to work on projects that span the boundaries of traditional disciplines. It emphasizes collaboration, experimentation, and the creation of new tools and technologies.\n\nWhat is the MIT Media Lab's impact?\n\nThe MIT Media Lab has had a significant impact on a wide range of fields, including computer science, engineering, design, art, and media. It has produced numerous innovations and technologies that have had a profound impact on society, including the World Wide Web, the first digital camera, and the first wearable computer.\n\nWhat is the MIT Media Lab's relationship with industry?\n\nThe MIT Media Lab has a strong relationship with industry, with many of its projects and innovations being commercialized by startups and established companies. It also has a number of industry partnerships and collaborations, and many of its faculty members and researchers have ties to industry.\n\nWhat is the MIT Media Lab's relationship with academia?\n\nThe MIT Media Lab has a strong relationship with academia, with many of its faculty members and researchers being affiliated with other departments and research centers at MIT. It also collaborates with universities and research institutions around the world.\n\nWhat is the MIT Media Lab's relationship with government?\n\nThe MIT Media Lab has a number of government partnerships and collaborations, including with the National Science Foundation, the National Institutes of Health, and the Department of Defense. It also works on projects related to national security, defense, and public policy.\n\nWhat is the MIT Media Lab's relationship with the public?\n\nThe MIT Media Lab has a strong commitment to public engagement and outreach, with many of its projects and innovations being made available<|end_of_text|>\n<|start_of_role|>certainty<|end_of_role|>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [8197, 438, 7131, 49, 203, 203, 6675, 438, 322, 28051, 867, 43017, 1421, 27358, 432, 21909, 30, 312, 945, 13234, 707, 9190, 328, 31719, 12154, 30, 28051, 867, 43017, 1421, 32, 2030, 438, 8967, 436, 2819, 12101, 18335, 328, 27536, 30, 33627, 30, 461, 21519, 30, 619, 4487, 619, 2819, 12480, 22557, 544, 328, 38795, 461, 17817, 1001, 941, 305, 4798, 32, 203, 203, 8197, 438, 322, 7131, 9821, 21905, 49, 203, 203, 1318, 7131, 9821, 21905, 438, 312, 13234, 20173, 8405, 4797, 7131, 688, 35167, 7668, 544, 322, 20877, 432, 21519, 30, 5549, 30, 6735, 30, 461, 27536, 32, 2030, 1597, 16870, 6918, 328, 225, 35, 43, 42, 39, 461, 438, 8967, 436, 2819, 1426, 1285, 22257, 3172, 10078, 372, 13234, 461, 2819, 12480, 22557, 544, 39973, 461, 328, 38795, 32, 203, 203, 8197, 438, 322, 7131, 9821, 21905, 1182, 28860, 49, 203, 203, 1318, 7131, 9821, 21905, 1182, 28860, 438, 372, 26826, 322, 457, 23765, 432, 5549, 30, 6735, 30, 27536, 30, 461, 21519, 32, 2030, 43784, 372, 1487, 537, 7444, 461, 30869, 688, 883, 526, 1654, 372, 2965, 8640, 9808, 461, 14642, 322, 13462, 3700, 32, 203, 203, 8197, 438, 322, 7131, 9821, 21905, 1182, 10078, 372, 13234, 49, 203, 203, 1318, 7131, 9821, 21905, 8727, 600, 1426, 1285, 22257, 3172, 10078, 372, 13234, 30, 47867, 10752, 538, 44361, 645, 312, 16264, 2155, 432, 3829, 372, 1389, 544, 8528, 688, 8106, 322, 28116, 432, 35899, 1214, 22257, 2353, 32, 2030, 12480, 2040, 5052, 39973, 30, 9624, 367, 30, 461, 322, 10064, 432, 537, 8684, 461, 30869, 32, 203, 203, 8197, 438, 322, 7131, 9821, 21905, 1182, 14932, 49, 203, 203, 1318, 7131, 9821, 21905, 1401, 5054, 312, 14724, 14932, 544, 312, 16264, 2155, 432, 3829, 30, 6237, 10670, 27536, 30, 33627, 30, 6735, 30, 5549, 30, 461, 7444, 32, 2030, 1401, 16500, 21639, 352, 328, 15007, 993, 461, 30869, 688, 1159, 5054, 312, 534, 6264, 14932, 544, 309, 24762, 30, 6237, 322, 10896, 25321, 3704, 30, 322, 1932, 18452, 8621, 30, 461, 322, 1932, 996, 18035, 10670, 32, 203, 203, 8197, 438, 322, 7131, 9821, 21905, 1182, 12112, 623, 29404, 49, 203, 203, 1318, 7131, 9821, 21905, 1401, 312, 12101, 12112, 623, 29404, 30, 623, 5075, 432, 2819, 8528, 461, 328, 15007, 993, 3998, 27663, 1191, 810, 1477, 9016, 461, 27493, 27524, 32, 2030, 2329, 1401, 312, 1451, 432, 29404, 1742, 96, 10082, 101, 461, 21368, 993, 30, 461, 5075, 432, 2819, 9652, 31146, 8999, 461, 13234, 483, 1159, 273, 732, 372, 29404, 32, 203, 203, 8197, 438, 322, 7131, 9821, 21905, 1182, 12112, 623, 312, 9002, 95, 905, 49, 203, 203, 1318, 7131, 9821, 21905, 1401, 312, 12101, 12112, 623, 312, 9002, 95, 905, 30, 623, 5075, 432, 2819, 9652, 31146, 8999, 461, 13234, 483, 3998, 19964, 733, 623, 1604, 26811, 1728, 461, 13234, 40962, 821, 7131, 32, 2030, 2329, 21368, 1196, 623, 707, 5810, 2105, 461, 13234, 31681, 101, 6835, 322, 5788, 32, 203, 203, 8197, 438, 322, 7131, 9821, 21905, 1182, 12112, 623, 31789, 49, 203, 203, 1318, 7131, 9821, 21905, 1401, 312, 1451, 432, 31789, 1742, 96, 10082, 101, 461, 21368, 993, 30, 6237, 623, 322, 24106, 19519, 7113, 30, 322, 24106, 4273, 283, 3676, 432, 19256, 30, 461, 322, 24506, 432, 3071, 838, 32, 2030, 2329, 4847, 544, 8528, 5886, 372, 32770, 7663, 30, 665, 838, 30, 461, 562, 7231, 32, 203, 203, 8197, 438, 322, 7131, 9821, 21905, 1182, 12112, 623, 322, 562, 49, 203, 203, 1318, 7131, 9821, 21905, 1401, 312, 12101, 4583, 469, 372, 562, 14707, 3877, 461, 963, 14708, 30, 623, 5075, 432, 2819, 8528, 461, 328, 15007, 993, 3998, 5590, 3304, 0, 203, 49152, 6989, 24933, 49153], lora_request: LoRARequest(lora_name='new_alora', lora_int_id=1, lora_path='/proj/dmfexp/statllm/users/kgreenewald/.cache/huggingface/models/hub/models--ibm-granite--granite-3.2-8b-alora-uncertainty/snapshots/6109ad88201426003e696d023ec67c19e7f3d444', lora_local_path=None, long_lora_max_len=None, base_model_name='ibm-granite/granite-3.2-8b-instruct'), prompt_adapter_request: None.
INFO 05-28 20:30:45 [logger.py:39] Received request cmpl-85bd61a9372e4a71b367e4c47e5d5cf2-1: prompt: '<|start_of_role|>user<|end_of_role|>What is MIT?<|end_of_text|>\n\n1. MIT, or Massachusetts Institute of Technology, is a prestigious private research university located in Cambridge, Massachusetts, USA.\n2. It was founded in 1861 and is known for its strong programs in science, engineering, and technology.\n3. MIT is often ranked as one of the top universities globally and is a member of the Ivy League.\n4. The university is renowned for its innovative research and contributions to various fields, including computer science, physics, and economics.\n5. Notable alumni include physicist Richard Feynman, computer scientist Marvin Minsky, and entrepreneur Elon Musk.<|end_of_text|>\n<|start_of_role|>certainty<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [49152, 496, 49153, 8197, 438, 7131, 49, 0, 203, 203, 35, 32, 7131, 30, 556, 28051, 867, 43017, 1421, 27358, 432, 21909, 30, 438, 312, 41804, 365, 2406, 945, 13234, 707, 9190, 15282, 328, 31719, 12154, 30, 28051, 867, 43017, 1421, 30, 15070, 32, 203, 36, 32, 2030, 1597, 16870, 6918, 328, 225, 35, 42, 40, 35, 461, 438, 8967, 436, 2819, 12101, 18335, 328, 27536, 30, 33627, 30, 461, 21519, 32, 203, 37, 32, 7131, 438, 12270, 11263, 318, 619, 1591, 432, 322, 2663, 707, 5810, 2105, 29585, 461, 438, 312, 5809, 432, 322, 439, 13331, 45223, 32, 203, 38, 32, 886, 707, 9190, 438, 316, 2481, 318, 436, 2819, 328, 15007, 1353, 13234, 461, 25198, 372, 10297, 3829, 30, 6237, 10670, 27536, 30, 28231, 30, 461, 32848, 1316, 32, 203, 39, 32, 3182, 444, 743, 1129, 91, 2305, 26571, 295, 427, 39042, 15311, 841, 1588, 30, 10670, 2197, 1606, 427, 8359, 13762, 7507, 14265, 30, 461, 17817, 1001, 941, 305, 516, 9512, 488, 352, 93, 32, 0, 203, 49152, 6989, 24933, 49153], lora_request: LoRARequest(lora_name='new_alora', lora_int_id=1, lora_path='/proj/dmfexp/statllm/users/kgreenewald/.cache/huggingface/models/hub/models--ibm-granite--granite-3.2-8b-alora-uncertainty/snapshots/6109ad88201426003e696d023ec67c19e7f3d444', lora_local_path=None, long_lora_max_len=None, base_model_name='ibm-granite/granite-3.2-8b-instruct'), prompt_adapter_request: None.
INFO 05-28 20:30:45 [async_llm.py:255] Added request cmpl-85bd61a9372e4a71b367e4c47e5d5cf2-0.
INFO 05-28 20:30:45 [async_llm.py:255] Added request cmpl-85bd61a9372e4a71b367e4c47e5d5cf2-1.
INFO:     127.0.0.1:48236 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-28 20:30:50 [loggers.py:116] Engine 000: Avg prompt throughput: 78.4 tokens/s, Avg generation throughput: 36.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 05-28 20:31:00 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 05-28 20:31:34 [logger.py:39] Received request cmpl-b996e19a236b4b0f9c44d3252aabda11-0: prompt: 'What is MIT?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=600, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [8197, 438, 7131, 49], lora_request: None, prompt_adapter_request: None.
INFO 05-28 20:31:34 [logger.py:39] Received request cmpl-b996e19a236b4b0f9c44d3252aabda11-1: prompt: '<|start_of_role|>user<|end_of_role|>What is MIT?<|end_of_text|>\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=600, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [49152, 496, 49153, 8197, 438, 7131, 49, 0, 203], lora_request: None, prompt_adapter_request: None.
INFO 05-28 20:31:34 [async_llm.py:255] Added request cmpl-b996e19a236b4b0f9c44d3252aabda11-0.
INFO 05-28 20:31:34 [async_llm.py:255] Added request cmpl-b996e19a236b4b0f9c44d3252aabda11-1.
INFO 05-28 20:31:40 [loggers.py:116] Engine 000: Avg prompt throughput: 1.3 tokens/s, Avg generation throughput: 46.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
INFO:     127.0.0.1:43158 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-28 20:31:44 [logger.py:39] Received request cmpl-82c02502e1084f9ba64c4adef5fbc7c0-0: prompt: "What is MIT?\n\nMIT is the Massachusetts Institute of Technology, a private research university in Cambridge, Massachusetts. It is known for its strong programs in science, engineering, and technology, as well as its emphasis on innovation and entrepreneurship.\n\nWhat is the MIT Media Lab?\n\nThe MIT Media Lab is a research laboratory within MIT that focuses on the intersection of technology, art, design, and science. It was founded in 1985 and is known for its interdisciplinary approach to research and its emphasis on collaboration and innovation.\n\nWhat is the MIT Media Lab's mission?\n\nThe MIT Media Lab's mission is to explore the confluence of art, design, science, and technology. It aims to create new media and technologies that can be used to address complex problems and improve the human condition.\n\nWhat is the MIT Media Lab's approach to research?\n\nThe MIT Media Lab takes an interdisciplinary approach to research, bringing together experts from a wide range of fields to work on projects that span the boundaries of traditional disciplines. It emphasizes collaboration, experimentation, and the creation of new tools and technologies.\n\nWhat is the MIT Media Lab's impact?\n\nThe MIT Media Lab has had a significant impact on a wide range of fields, including computer science, engineering, design, art, and media. It has produced numerous innovations and technologies that have had a profound impact on society, including the World Wide Web, the first digital camera, and the first wearable computer.\n\nWhat is the MIT Media Lab's relationship with industry?\n\nThe MIT Media Lab has a strong relationship with industry, with many of its projects and innovations being commercialized by startups and established companies. It also has a number of industry partnerships and collaborations, and many of its faculty members and researchers have ties to industry.\n\nWhat is the MIT Media Lab's relationship with academia?\n\nThe MIT Media Lab has a strong relationship with academia, with many of its faculty members and researchers being affiliated with other departments and research centers at MIT. It also collaborates with universities and research institutions around the world.\n\nWhat is the MIT Media Lab's relationship with government?\n\nThe MIT Media Lab has a number of government partnerships and collaborations, including with the National Science Foundation, the National Institutes of Health, and the Department of Defense. It also works on projects related to national security, defense, and public policy.\n\nWhat is the MIT Media Lab's relationship with the public?\n\nThe MIT Media Lab has a strong commitment to public engagement and outreach, with many of its projects and innovations being made available<|end_of_text|>\n<|start_of_role|>certainty<|end_of_role|>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [8197, 438, 7131, 49, 203, 203, 6675, 438, 322, 28051, 867, 43017, 1421, 27358, 432, 21909, 30, 312, 945, 13234, 707, 9190, 328, 31719, 12154, 30, 28051, 867, 43017, 1421, 32, 2030, 438, 8967, 436, 2819, 12101, 18335, 328, 27536, 30, 33627, 30, 461, 21519, 30, 619, 4487, 619, 2819, 12480, 22557, 544, 328, 38795, 461, 17817, 1001, 941, 305, 4798, 32, 203, 203, 8197, 438, 322, 7131, 9821, 21905, 49, 203, 203, 1318, 7131, 9821, 21905, 438, 312, 13234, 20173, 8405, 4797, 7131, 688, 35167, 7668, 544, 322, 20877, 432, 21519, 30, 5549, 30, 6735, 30, 461, 27536, 32, 2030, 1597, 16870, 6918, 328, 225, 35, 43, 42, 39, 461, 438, 8967, 436, 2819, 1426, 1285, 22257, 3172, 10078, 372, 13234, 461, 2819, 12480, 22557, 544, 39973, 461, 328, 38795, 32, 203, 203, 8197, 438, 322, 7131, 9821, 21905, 1182, 28860, 49, 203, 203, 1318, 7131, 9821, 21905, 1182, 28860, 438, 372, 26826, 322, 457, 23765, 432, 5549, 30, 6735, 30, 27536, 30, 461, 21519, 32, 2030, 43784, 372, 1487, 537, 7444, 461, 30869, 688, 883, 526, 1654, 372, 2965, 8640, 9808, 461, 14642, 322, 13462, 3700, 32, 203, 203, 8197, 438, 322, 7131, 9821, 21905, 1182, 10078, 372, 13234, 49, 203, 203, 1318, 7131, 9821, 21905, 8727, 600, 1426, 1285, 22257, 3172, 10078, 372, 13234, 30, 47867, 10752, 538, 44361, 645, 312, 16264, 2155, 432, 3829, 372, 1389, 544, 8528, 688, 8106, 322, 28116, 432, 35899, 1214, 22257, 2353, 32, 2030, 12480, 2040, 5052, 39973, 30, 9624, 367, 30, 461, 322, 10064, 432, 537, 8684, 461, 30869, 32, 203, 203, 8197, 438, 322, 7131, 9821, 21905, 1182, 14932, 49, 203, 203, 1318, 7131, 9821, 21905, 1401, 5054, 312, 14724, 14932, 544, 312, 16264, 2155, 432, 3829, 30, 6237, 10670, 27536, 30, 33627, 30, 6735, 30, 5549, 30, 461, 7444, 32, 2030, 1401, 16500, 21639, 352, 328, 15007, 993, 461, 30869, 688, 1159, 5054, 312, 534, 6264, 14932, 544, 309, 24762, 30, 6237, 322, 10896, 25321, 3704, 30, 322, 1932, 18452, 8621, 30, 461, 322, 1932, 996, 18035, 10670, 32, 203, 203, 8197, 438, 322, 7131, 9821, 21905, 1182, 12112, 623, 29404, 49, 203, 203, 1318, 7131, 9821, 21905, 1401, 312, 12101, 12112, 623, 29404, 30, 623, 5075, 432, 2819, 8528, 461, 328, 15007, 993, 3998, 27663, 1191, 810, 1477, 9016, 461, 27493, 27524, 32, 2030, 2329, 1401, 312, 1451, 432, 29404, 1742, 96, 10082, 101, 461, 21368, 993, 30, 461, 5075, 432, 2819, 9652, 31146, 8999, 461, 13234, 483, 1159, 273, 732, 372, 29404, 32, 203, 203, 8197, 438, 322, 7131, 9821, 21905, 1182, 12112, 623, 312, 9002, 95, 905, 49, 203, 203, 1318, 7131, 9821, 21905, 1401, 312, 12101, 12112, 623, 312, 9002, 95, 905, 30, 623, 5075, 432, 2819, 9652, 31146, 8999, 461, 13234, 483, 3998, 19964, 733, 623, 1604, 26811, 1728, 461, 13234, 40962, 821, 7131, 32, 2030, 2329, 21368, 1196, 623, 707, 5810, 2105, 461, 13234, 31681, 101, 6835, 322, 5788, 32, 203, 203, 8197, 438, 322, 7131, 9821, 21905, 1182, 12112, 623, 31789, 49, 203, 203, 1318, 7131, 9821, 21905, 1401, 312, 1451, 432, 31789, 1742, 96, 10082, 101, 461, 21368, 993, 30, 6237, 623, 322, 24106, 19519, 7113, 30, 322, 24106, 4273, 283, 3676, 432, 19256, 30, 461, 322, 24506, 432, 3071, 838, 32, 2030, 2329, 4847, 544, 8528, 5886, 372, 32770, 7663, 30, 665, 838, 30, 461, 562, 7231, 32, 203, 203, 8197, 438, 322, 7131, 9821, 21905, 1182, 12112, 623, 322, 562, 49, 203, 203, 1318, 7131, 9821, 21905, 1401, 312, 12101, 4583, 469, 372, 562, 14707, 3877, 461, 963, 14708, 30, 623, 5075, 432, 2819, 8528, 461, 328, 15007, 993, 3998, 5590, 3304, 0, 203, 49152, 6989, 24933, 49153], lora_request: LoRARequest(lora_name='new_alora', lora_int_id=1, lora_path='/proj/dmfexp/statllm/users/kgreenewald/.cache/huggingface/models/hub/models--ibm-granite--granite-3.2-8b-alora-uncertainty/snapshots/6109ad88201426003e696d023ec67c19e7f3d444', lora_local_path=None, long_lora_max_len=None, base_model_name='ibm-granite/granite-3.2-8b-instruct'), prompt_adapter_request: None.
INFO 05-28 20:31:44 [logger.py:39] Received request cmpl-82c02502e1084f9ba64c4adef5fbc7c0-1: prompt: '<|start_of_role|>user<|end_of_role|>What is MIT?<|end_of_text|>\n\n1. MIT, or Massachusetts Institute of Technology, is a prestigious private research university located in Cambridge, Massachusetts, USA.\n2. It was founded in 1861 and is known for its strong programs in science, engineering, and technology.\n3. MIT is often ranked as one of the top universities globally and is a member of the Ivy League.\n4. The university is renowned for its innovative research and contributions to various fields, including computer science, physics, and economics.\n5. Notable alumni include physicist Richard Feynman, computer scientist Marvin Minsky, and entrepreneur Elon Musk.<|end_of_text|>\n<|start_of_role|>certainty<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [49152, 496, 49153, 8197, 438, 7131, 49, 0, 203, 203, 35, 32, 7131, 30, 556, 28051, 867, 43017, 1421, 27358, 432, 21909, 30, 438, 312, 41804, 365, 2406, 945, 13234, 707, 9190, 15282, 328, 31719, 12154, 30, 28051, 867, 43017, 1421, 30, 15070, 32, 203, 36, 32, 2030, 1597, 16870, 6918, 328, 225, 35, 42, 40, 35, 461, 438, 8967, 436, 2819, 12101, 18335, 328, 27536, 30, 33627, 30, 461, 21519, 32, 203, 37, 32, 7131, 438, 12270, 11263, 318, 619, 1591, 432, 322, 2663, 707, 5810, 2105, 29585, 461, 438, 312, 5809, 432, 322, 439, 13331, 45223, 32, 203, 38, 32, 886, 707, 9190, 438, 316, 2481, 318, 436, 2819, 328, 15007, 1353, 13234, 461, 25198, 372, 10297, 3829, 30, 6237, 10670, 27536, 30, 28231, 30, 461, 32848, 1316, 32, 203, 39, 32, 3182, 444, 743, 1129, 91, 2305, 26571, 295, 427, 39042, 15311, 841, 1588, 30, 10670, 2197, 1606, 427, 8359, 13762, 7507, 14265, 30, 461, 17817, 1001, 941, 305, 516, 9512, 488, 352, 93, 32, 0, 203, 49152, 6989, 24933, 49153], lora_request: LoRARequest(lora_name='new_alora', lora_int_id=1, lora_path='/proj/dmfexp/statllm/users/kgreenewald/.cache/huggingface/models/hub/models--ibm-granite--granite-3.2-8b-alora-uncertainty/snapshots/6109ad88201426003e696d023ec67c19e7f3d444', lora_local_path=None, long_lora_max_len=None, base_model_name='ibm-granite/granite-3.2-8b-instruct'), prompt_adapter_request: None.
INFO 05-28 20:31:44 [async_llm.py:255] Added request cmpl-82c02502e1084f9ba64c4adef5fbc7c0-0.
INFO 05-28 20:31:44 [async_llm.py:255] Added request cmpl-82c02502e1084f9ba64c4adef5fbc7c0-1.
INFO:     127.0.0.1:43158 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-28 20:31:50 [loggers.py:116] Engine 000: Avg prompt throughput: 78.4 tokens/s, Avg generation throughput: 31.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 05-28 20:32:00 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 05-28 20:33:48 [launcher.py:79] Shutting down FastAPI HTTP server.
[rank0]:[W528 20:33:49.688683453 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
INFO 05-28 20:34:25 [__init__.py:248] Automatically detected platform cuda.
INFO 05-28 20:34:28 [api_server.py:1042] vLLM API server version 0.1.dev6336+gbe40f32
INFO 05-28 20:34:28 [api_server.py:1043] args: Namespace(subparser='serve', model_tag='ibm-granite/granite-3.2-8b-instruct', config='', host=None, port=8000, uvicorn_log_level='info', disable_uvicorn_access_log=False, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='ibm-granite/granite-3.2-8b-instruct', task='auto', tokenizer=None, tokenizer_mode='auto', trust_remote_code=False, dtype='auto', seed=None, hf_config_path=None, allowed_local_media_path='', revision=None, code_revision=None, rope_scaling={}, rope_theta=None, tokenizer_revision=None, max_model_len=None, quantization=None, enforce_eager=False, max_seq_len_to_capture=8192, max_logprobs=20, disable_sliding_window=False, disable_cascade_attn=False, skip_tokenizer_init=False, enable_prompt_embeds=False, served_model_name=None, disable_async_output_proc=False, config_format='auto', hf_token=None, hf_overrides={}, override_neuron_config={}, override_pooler_config=None, logits_processor_pattern=None, generation_config='auto', override_generation_config={}, enable_sleep_mode=False, model_impl='auto', load_format='auto', download_dir=None, model_loader_extra_config={}, ignore_patterns=None, use_tqdm_on_load=True, qlora_adapter_name_or_path=None, pt_load_map_location='cpu', guided_decoding_backend='auto', guided_decoding_disable_fallback=False, guided_decoding_disable_any_whitespace=False, guided_decoding_disable_additional_properties=False, enable_reasoning=None, reasoning_parser='', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, data_parallel_size=1, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, disable_custom_all_reduce=False, worker_cls='auto', worker_extension_cls='', block_size=None, gpu_memory_utilization=0.9, swap_space=4, kv_cache_dtype='auto', num_gpu_blocks_override=None, enable_prefix_caching=None, prefix_caching_hash_algo='builtin', cpu_offload_gb=0, calculate_kv_scales=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config={}, limit_mm_per_prompt={}, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=True, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=None, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', speculative_config=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, max_num_batched_tokens=None, max_num_seqs=None, max_num_partial_prefills=1, max_long_partial_prefills=1, cuda_graph_sizes=[512], long_prefill_token_threshold=0, num_lookahead_slots=0, scheduler_delay_factor=0.0, preemption_mode=None, num_scheduler_steps=1, multi_step_stream_outputs=True, scheduling_policy='fcfs', enable_chunked_prefill=None, disable_chunked_mm_input=False, scheduler_cls='vllm.core.scheduler.Scheduler', compilation_config=None, kv_transfer_config=None, kv_events_config=None, additional_config=None, use_v2_block_manager=True, disable_log_stats=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False, dispatch_function=<function ServeSubcommand.cmd at 0x14840cdac400>)
INFO 05-28 20:34:34 [config.py:752] This model supports multiple tasks: {'generate', 'classify', 'embed', 'reward', 'score'}. Defaulting to 'generate'.
INFO 05-28 20:34:34 [config.py:2057] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 05-28 20:34:37 [__init__.py:248] Automatically detected platform cuda.
INFO 05-28 20:34:39 [core.py:59] Initializing a V1 LLM engine (v0.1.dev6336+gbe40f32) with config: model='ibm-granite/granite-3.2-8b-instruct', speculative_config=None, tokenizer='ibm-granite/granite-3.2-8b-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=ibm-granite/granite-3.2-8b-instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
WARNING 05-28 20:34:39 [utils.py:2621] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x155056bf02c0>
INFO 05-28 20:34:40 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 05-28 20:34:40 [cuda.py:221] Using Flash Attention backend on V1 engine.
WARNING 05-28 20:34:40 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
INFO 05-28 20:34:40 [gpu_model_runner.py:1436] Starting to load model ibm-granite/granite-3.2-8b-instruct...
INFO 05-28 20:34:40 [weight_utils.py:257] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:07,  2.45s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:04<00:04,  2.09s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:04<00:01,  1.39s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:06<00:00,  1.54s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:06<00:00,  1.66s/it]

INFO 05-28 20:34:47 [default_loader.py:278] Loading weights took 6.70 seconds
INFO 05-28 20:34:47 [punica_selector.py:18] Using PunicaWrapperGPU.
INFO 05-28 20:34:48 [gpu_model_runner.py:1454] Model loading took 15.3529 GiB and 7.285306 seconds
INFO 05-28 20:34:58 [backends.py:437] Using cache directory: /u/kgreenewald/.cache/vllm/torch_compile_cache/f5e2b8507e/rank_0_0 for vLLM's torch.compile
INFO 05-28 20:34:58 [backends.py:447] Dynamo bytecode transform time: 10.41 s
INFO 05-28 20:35:03 [backends.py:138] Cache the graph of shape None for later use
INFO 05-28 20:35:39 [backends.py:150] Compiling a graph for general shape takes 39.83 s
INFO 05-28 20:36:02 [monitor.py:33] torch.compile takes 50.23 s in total
INFO 05-28 20:36:03 [kv_cache_utils.py:643] GPU KV cache size: 349,360 tokens
INFO 05-28 20:36:03 [kv_cache_utils.py:646] Maximum concurrency for 131,072 tokens per request: 2.67x
INFO 05-28 20:37:06 [gpu_model_runner.py:1798] Graph capturing finished in 64 secs, took 1.07 GiB
INFO 05-28 20:37:07 [core.py:161] init engine (profile, create kv cache, warmup model) took 139.31 seconds
INFO 05-28 20:37:07 [core_client.py:442] Core engine process 0 ready.
INFO 05-28 20:37:07 [loggers.py:136] vllm cache_config_info with initialization after num_gpu_blocks is: 21835
INFO 05-28 20:37:07 [api_server.py:1089] Starting vLLM API server on http://0.0.0.0:8000
INFO 05-28 20:37:07 [launcher.py:28] Available routes are:
INFO 05-28 20:37:07 [launcher.py:36] Route: /openapi.json, Methods: HEAD, GET
INFO 05-28 20:37:07 [launcher.py:36] Route: /docs, Methods: HEAD, GET
INFO 05-28 20:37:07 [launcher.py:36] Route: /docs/oauth2-redirect, Methods: HEAD, GET
INFO 05-28 20:37:07 [launcher.py:36] Route: /redoc, Methods: HEAD, GET
INFO 05-28 20:37:07 [launcher.py:36] Route: /health, Methods: GET
INFO 05-28 20:37:07 [launcher.py:36] Route: /load, Methods: GET
INFO 05-28 20:37:07 [launcher.py:36] Route: /ping, Methods: GET, POST
INFO 05-28 20:37:07 [launcher.py:36] Route: /tokenize, Methods: POST
INFO 05-28 20:37:07 [launcher.py:36] Route: /detokenize, Methods: POST
INFO 05-28 20:37:07 [launcher.py:36] Route: /v1/models, Methods: GET
INFO 05-28 20:37:07 [launcher.py:36] Route: /version, Methods: GET
INFO 05-28 20:37:07 [launcher.py:36] Route: /v1/chat/completions, Methods: POST
INFO 05-28 20:37:07 [launcher.py:36] Route: /v1/completions, Methods: POST
INFO 05-28 20:37:07 [launcher.py:36] Route: /v1/embeddings, Methods: POST
INFO 05-28 20:37:07 [launcher.py:36] Route: /pooling, Methods: POST
INFO 05-28 20:37:07 [launcher.py:36] Route: /score, Methods: POST
INFO 05-28 20:37:07 [launcher.py:36] Route: /v1/score, Methods: POST
INFO 05-28 20:37:07 [launcher.py:36] Route: /v1/audio/transcriptions, Methods: POST
INFO 05-28 20:37:07 [launcher.py:36] Route: /rerank, Methods: POST
INFO 05-28 20:37:07 [launcher.py:36] Route: /v1/rerank, Methods: POST
INFO 05-28 20:37:07 [launcher.py:36] Route: /v2/rerank, Methods: POST
INFO 05-28 20:37:07 [launcher.py:36] Route: /invocations, Methods: POST
INFO 05-28 20:37:07 [launcher.py:36] Route: /metrics, Methods: GET
INFO:     Started server process [2513230]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO 05-28 20:37:16 [launcher.py:79] Shutting down FastAPI HTTP server.
[rank0]:[W528 20:37:16.863415878 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
examples/alora/alora_server_testing.sh: line 9: --lora-modules: command not found
INFO 05-28 20:39:39 [__init__.py:248] Automatically detected platform cuda.
INFO 05-28 20:39:42 [api_server.py:1042] vLLM API server version 0.1.dev6336+gbe40f32
INFO 05-28 20:39:42 [api_server.py:1043] args: Namespace(subparser='serve', model_tag='ibm-granite/granite-3.2-8b-instruct', config='', host=None, port=8000, uvicorn_log_level='info', disable_uvicorn_access_log=False, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='ibm-granite/granite-3.2-8b-instruct', task='auto', tokenizer=None, tokenizer_mode='auto', trust_remote_code=False, dtype='auto', seed=None, hf_config_path=None, allowed_local_media_path='', revision=None, code_revision=None, rope_scaling={}, rope_theta=None, tokenizer_revision=None, max_model_len=None, quantization=None, enforce_eager=False, max_seq_len_to_capture=8192, max_logprobs=20, disable_sliding_window=False, disable_cascade_attn=False, skip_tokenizer_init=False, enable_prompt_embeds=False, served_model_name=None, disable_async_output_proc=False, config_format='auto', hf_token=None, hf_overrides={}, override_neuron_config={}, override_pooler_config=None, logits_processor_pattern=None, generation_config='auto', override_generation_config={}, enable_sleep_mode=False, model_impl='auto', load_format='auto', download_dir=None, model_loader_extra_config={}, ignore_patterns=None, use_tqdm_on_load=True, qlora_adapter_name_or_path=None, pt_load_map_location='cpu', guided_decoding_backend='auto', guided_decoding_disable_fallback=False, guided_decoding_disable_any_whitespace=False, guided_decoding_disable_additional_properties=False, enable_reasoning=None, reasoning_parser='', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, data_parallel_size=1, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, disable_custom_all_reduce=False, worker_cls='auto', worker_extension_cls='', block_size=None, gpu_memory_utilization=0.9, swap_space=4, kv_cache_dtype='auto', num_gpu_blocks_override=None, enable_prefix_caching=None, prefix_caching_hash_algo='builtin', cpu_offload_gb=0, calculate_kv_scales=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config={}, limit_mm_per_prompt={}, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=True, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=None, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', speculative_config=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, max_num_batched_tokens=None, max_num_seqs=None, max_num_partial_prefills=1, max_long_partial_prefills=1, cuda_graph_sizes=[512], long_prefill_token_threshold=0, num_lookahead_slots=0, scheduler_delay_factor=0.0, preemption_mode=None, num_scheduler_steps=1, multi_step_stream_outputs=True, scheduling_policy='fcfs', enable_chunked_prefill=None, disable_chunked_mm_input=False, scheduler_cls='vllm.core.scheduler.Scheduler', compilation_config=None, kv_transfer_config=None, kv_events_config=None, additional_config=None, use_v2_block_manager=True, disable_log_stats=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False, dispatch_function=<function ServeSubcommand.cmd at 0x14f3e6de4400>)
INFO 05-28 20:39:48 [config.py:752] This model supports multiple tasks: {'embed', 'reward', 'classify', 'generate', 'score'}. Defaulting to 'generate'.
INFO 05-28 20:39:48 [config.py:2057] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 05-28 20:39:52 [__init__.py:248] Automatically detected platform cuda.
INFO 05-28 20:39:54 [core.py:59] Initializing a V1 LLM engine (v0.1.dev6336+gbe40f32) with config: model='ibm-granite/granite-3.2-8b-instruct', speculative_config=None, tokenizer='ibm-granite/granite-3.2-8b-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=ibm-granite/granite-3.2-8b-instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
WARNING 05-28 20:39:54 [utils.py:2621] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x14dd5a4e20f0>
INFO 05-28 20:39:55 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 05-28 20:39:55 [cuda.py:221] Using Flash Attention backend on V1 engine.
WARNING 05-28 20:39:55 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
INFO 05-28 20:39:55 [gpu_model_runner.py:1436] Starting to load model ibm-granite/granite-3.2-8b-instruct...
INFO 05-28 20:39:56 [weight_utils.py:257] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:05,  1.76s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:03<00:03,  1.80s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:04<00:01,  1.23s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:05<00:00,  1.44s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:05<00:00,  1.48s/it]

INFO 05-28 20:40:02 [default_loader.py:278] Loading weights took 5.99 seconds
INFO 05-28 20:40:02 [punica_selector.py:18] Using PunicaWrapperGPU.
INFO 05-28 20:40:02 [gpu_model_runner.py:1454] Model loading took 15.3529 GiB and 6.548333 seconds
INFO 05-28 20:40:11 [backends.py:437] Using cache directory: /u/kgreenewald/.cache/vllm/torch_compile_cache/f5e2b8507e/rank_0_0 for vLLM's torch.compile
INFO 05-28 20:40:11 [backends.py:447] Dynamo bytecode transform time: 8.77 s
INFO 05-28 20:40:17 [backends.py:119] Directly load the compiled graph(s) for shape None from the cache, took 5.538 s
INFO 05-28 20:40:19 [monitor.py:33] torch.compile takes 8.77 s in total
INFO 05-28 20:40:19 [kv_cache_utils.py:643] GPU KV cache size: 349,472 tokens
INFO 05-28 20:40:19 [kv_cache_utils.py:646] Maximum concurrency for 131,072 tokens per request: 2.67x
INFO 05-28 20:40:41 [__init__.py:248] Automatically detected platform cuda.
INFO 05-28 20:40:45 [api_server.py:1042] vLLM API server version 0.1.dev6336+gbe40f32
INFO 05-28 20:40:45 [api_server.py:1043] args: Namespace(subparser='serve', model_tag='ibm-granite/granite-3.2-8b-instruct', config='', host=None, port=8000, uvicorn_log_level='info', disable_uvicorn_access_log=False, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='ibm-granite/granite-3.2-8b-instruct', task='auto', tokenizer=None, tokenizer_mode='auto', trust_remote_code=False, dtype='auto', seed=None, hf_config_path=None, allowed_local_media_path='', revision=None, code_revision=None, rope_scaling={}, rope_theta=None, tokenizer_revision=None, max_model_len=None, quantization=None, enforce_eager=False, max_seq_len_to_capture=8192, max_logprobs=20, disable_sliding_window=False, disable_cascade_attn=False, skip_tokenizer_init=False, enable_prompt_embeds=False, served_model_name=None, disable_async_output_proc=False, config_format='auto', hf_token=None, hf_overrides={}, override_neuron_config={}, override_pooler_config=None, logits_processor_pattern=None, generation_config='auto', override_generation_config={}, enable_sleep_mode=False, model_impl='auto', load_format='auto', download_dir=None, model_loader_extra_config={}, ignore_patterns=None, use_tqdm_on_load=True, qlora_adapter_name_or_path=None, pt_load_map_location='cpu', guided_decoding_backend='auto', guided_decoding_disable_fallback=False, guided_decoding_disable_any_whitespace=False, guided_decoding_disable_additional_properties=False, enable_reasoning=None, reasoning_parser='', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, data_parallel_size=1, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, disable_custom_all_reduce=False, worker_cls='auto', worker_extension_cls='', block_size=None, gpu_memory_utilization=0.9, swap_space=4, kv_cache_dtype='auto', num_gpu_blocks_override=None, enable_prefix_caching=None, prefix_caching_hash_algo='builtin', cpu_offload_gb=0, calculate_kv_scales=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config={}, limit_mm_per_prompt={}, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=True, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=None, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', speculative_config=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, max_num_batched_tokens=None, max_num_seqs=None, max_num_partial_prefills=1, max_long_partial_prefills=1, cuda_graph_sizes=[512], long_prefill_token_threshold=0, num_lookahead_slots=0, scheduler_delay_factor=0.0, preemption_mode=None, num_scheduler_steps=1, multi_step_stream_outputs=True, scheduling_policy='fcfs', enable_chunked_prefill=None, disable_chunked_mm_input=False, scheduler_cls='vllm.core.scheduler.Scheduler', compilation_config=None, kv_transfer_config=None, kv_events_config=None, additional_config=None, use_v2_block_manager=True, disable_log_stats=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False, dispatch_function=<function ServeSubcommand.cmd at 0x147c74ef4400>)
INFO 05-28 20:40:52 [config.py:752] This model supports multiple tasks: {'generate', 'score', 'reward', 'embed', 'classify'}. Defaulting to 'generate'.
INFO 05-28 20:40:52 [config.py:2057] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 05-28 20:40:55 [__init__.py:248] Automatically detected platform cuda.
INFO 05-28 20:40:58 [core.py:59] Initializing a V1 LLM engine (v0.1.dev6336+gbe40f32) with config: model='ibm-granite/granite-3.2-8b-instruct', speculative_config=None, tokenizer='ibm-granite/granite-3.2-8b-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=ibm-granite/granite-3.2-8b-instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
WARNING 05-28 20:40:58 [utils.py:2621] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x14d9d4063fb0>
ERROR 05-28 20:40:58 [core.py:400] EngineCore failed to start.
ERROR 05-28 20:40:58 [core.py:400] Traceback (most recent call last):
ERROR 05-28 20:40:58 [core.py:400]   File "/proj/dmfexp/statllm/users/kgreenewald/Thermometer/vllm-fork/vllm/vllm/v1/engine/core.py", line 391, in run_engine_core
ERROR 05-28 20:40:58 [core.py:400]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 05-28 20:40:58 [core.py:400]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 05-28 20:40:58 [core.py:400]   File "/proj/dmfexp/statllm/users/kgreenewald/Thermometer/vllm-fork/vllm/vllm/v1/engine/core.py", line 333, in __init__
ERROR 05-28 20:40:58 [core.py:400]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 05-28 20:40:58 [core.py:400]   File "/proj/dmfexp/statllm/users/kgreenewald/Thermometer/vllm-fork/vllm/vllm/v1/engine/core.py", line 65, in __init__
ERROR 05-28 20:40:58 [core.py:400]     self.model_executor = executor_class(vllm_config)
ERROR 05-28 20:40:58 [core.py:400]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 05-28 20:40:58 [core.py:400]   File "/proj/dmfexp/statllm/users/kgreenewald/Thermometer/vllm-fork/vllm/vllm/executor/executor_base.py", line 52, in __init__
ERROR 05-28 20:40:58 [core.py:400]     self._init_executor()
ERROR 05-28 20:40:58 [core.py:400]   File "/proj/dmfexp/statllm/users/kgreenewald/Thermometer/vllm-fork/vllm/vllm/executor/uniproc_executor.py", line 46, in _init_executor
ERROR 05-28 20:40:58 [core.py:400]     self.collective_rpc("init_device")
ERROR 05-28 20:40:58 [core.py:400]   File "/proj/dmfexp/statllm/users/kgreenewald/Thermometer/vllm-fork/vllm/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 05-28 20:40:58 [core.py:400]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 05-28 20:40:58 [core.py:400]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 05-28 20:40:58 [core.py:400]   File "/proj/dmfexp/statllm/users/kgreenewald/Thermometer/vllm-fork/vllm/vllm/utils.py", line 2555, in run_method
ERROR 05-28 20:40:58 [core.py:400]     return func(*args, **kwargs)
ERROR 05-28 20:40:58 [core.py:400]            ^^^^^^^^^^^^^^^^^^^^^
ERROR 05-28 20:40:58 [core.py:400]   File "/proj/dmfexp/statllm/users/kgreenewald/Thermometer/vllm-fork/vllm/vllm/worker/worker_base.py", line 604, in init_device
ERROR 05-28 20:40:58 [core.py:400]     self.worker.init_device()  # type: ignore
ERROR 05-28 20:40:58 [core.py:400]     ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 05-28 20:40:58 [core.py:400]   File "/proj/dmfexp/statllm/users/kgreenewald/Thermometer/vllm-fork/vllm/vllm/v1/worker/gpu_worker.py", line 131, in init_device
ERROR 05-28 20:40:58 [core.py:400]     self.init_gpu_memory = torch.cuda.mem_get_info()[0]
ERROR 05-28 20:40:58 [core.py:400]                            ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 05-28 20:40:58 [core.py:400]   File "/proj/dmfexp/statllm/share/envs/IBMaloraVLLM/lib/python3.12/site-packages/torch/cuda/memory.py", line 836, in mem_get_info
ERROR 05-28 20:40:58 [core.py:400]     return torch.cuda.cudart().cudaMemGetInfo(device)
ERROR 05-28 20:40:58 [core.py:400]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 05-28 20:40:58 [core.py:400] RuntimeError: CUDA error: CUDA-capable device(s) is/are busy or unavailable
ERROR 05-28 20:40:58 [core.py:400] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
ERROR 05-28 20:40:58 [core.py:400] For debugging consider passing CUDA_LAUNCH_BLOCKING=1
ERROR 05-28 20:40:58 [core.py:400] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
ERROR 05-28 20:40:58 [core.py:400] 
Process EngineCore_0:
Traceback (most recent call last):
  File "/proj/dmfexp/statllm/share/envs/IBMaloraVLLM/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/proj/dmfexp/statllm/share/envs/IBMaloraVLLM/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/proj/dmfexp/statllm/users/kgreenewald/Thermometer/vllm-fork/vllm/vllm/v1/engine/core.py", line 404, in run_engine_core
    raise e
  File "/proj/dmfexp/statllm/users/kgreenewald/Thermometer/vllm-fork/vllm/vllm/v1/engine/core.py", line 391, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/dmfexp/statllm/users/kgreenewald/Thermometer/vllm-fork/vllm/vllm/v1/engine/core.py", line 333, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/proj/dmfexp/statllm/users/kgreenewald/Thermometer/vllm-fork/vllm/vllm/v1/engine/core.py", line 65, in __init__
    self.model_executor = executor_class(vllm_config)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/dmfexp/statllm/users/kgreenewald/Thermometer/vllm-fork/vllm/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/proj/dmfexp/statllm/users/kgreenewald/Thermometer/vllm-fork/vllm/vllm/executor/uniproc_executor.py", line 46, in _init_executor
    self.collective_rpc("init_device")
  File "/proj/dmfexp/statllm/users/kgreenewald/Thermometer/vllm-fork/vllm/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/dmfexp/statllm/users/kgreenewald/Thermometer/vllm-fork/vllm/vllm/utils.py", line 2555, in run_method
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/proj/dmfexp/statllm/users/kgreenewald/Thermometer/vllm-fork/vllm/vllm/worker/worker_base.py", line 604, in init_device
    self.worker.init_device()  # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/dmfexp/statllm/users/kgreenewald/Thermometer/vllm-fork/vllm/vllm/v1/worker/gpu_worker.py", line 131, in init_device
    self.init_gpu_memory = torch.cuda.mem_get_info()[0]
                           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/dmfexp/statllm/share/envs/IBMaloraVLLM/lib/python3.12/site-packages/torch/cuda/memory.py", line 836, in mem_get_info
    return torch.cuda.cudart().cudaMemGetInfo(device)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: CUDA error: CUDA-capable device(s) is/are busy or unavailable
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Traceback (most recent call last):
  File "/proj/dmfexp/statllm/share/envs/IBMaloraVLLM/bin/vllm", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/proj/dmfexp/statllm/users/kgreenewald/Thermometer/vllm-fork/vllm/vllm/entrypoints/cli/main.py", line 53, in main
    args.dispatch_function(args)
  File "/proj/dmfexp/statllm/users/kgreenewald/Thermometer/vllm-fork/vllm/vllm/entrypoints/cli/serve.py", line 27, in cmd
    uvloop.run(run_server(args))
  File "/proj/dmfexp/statllm/share/envs/IBMaloraVLLM/lib/python3.12/site-packages/uvloop/__init__.py", line 109, in run
    return __asyncio.run(
           ^^^^^^^^^^^^^^
  File "/proj/dmfexp/statllm/share/envs/IBMaloraVLLM/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/proj/dmfexp/statllm/share/envs/IBMaloraVLLM/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
  File "/proj/dmfexp/statllm/share/envs/IBMaloraVLLM/lib/python3.12/site-packages/uvloop/__init__.py", line 61, in wrapper
    return await main
           ^^^^^^^^^^
  File "/proj/dmfexp/statllm/users/kgreenewald/Thermometer/vllm-fork/vllm/vllm/entrypoints/openai/api_server.py", line 1077, in run_server
    async with build_async_engine_client(args) as engine_client:
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/dmfexp/statllm/share/envs/IBMaloraVLLM/lib/python3.12/contextlib.py", line 210, in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/proj/dmfexp/statllm/users/kgreenewald/Thermometer/vllm-fork/vllm/vllm/entrypoints/openai/api_server.py", line 146, in build_async_engine_client
    async with build_async_engine_client_from_engine_args(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/dmfexp/statllm/share/envs/IBMaloraVLLM/lib/python3.12/contextlib.py", line 210, in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/proj/dmfexp/statllm/users/kgreenewald/Thermometer/vllm-fork/vllm/vllm/entrypoints/openai/api_server.py", line 178, in build_async_engine_client_from_engine_args
    async_llm = AsyncLLM.from_vllm_config(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/dmfexp/statllm/users/kgreenewald/Thermometer/vllm-fork/vllm/vllm/v1/engine/async_llm.py", line 151, in from_vllm_config
    return cls(
           ^^^^
  File "/proj/dmfexp/statllm/users/kgreenewald/Thermometer/vllm-fork/vllm/vllm/v1/engine/async_llm.py", line 118, in __init__
    self.engine_core = core_client_class(
                       ^^^^^^^^^^^^^^^^^^
  File "/proj/dmfexp/statllm/users/kgreenewald/Thermometer/vllm-fork/vllm/vllm/v1/engine/core_client.py", line 649, in __init__
    super().__init__(
  File "/proj/dmfexp/statllm/users/kgreenewald/Thermometer/vllm-fork/vllm/vllm/v1/engine/core_client.py", line 400, in __init__
    self._wait_for_engine_startup()
  File "/proj/dmfexp/statllm/users/kgreenewald/Thermometer/vllm-fork/vllm/vllm/v1/engine/core_client.py", line 432, in _wait_for_engine_startup
    raise RuntimeError("Engine core initialization failed. "
RuntimeError: Engine core initialization failed. See root cause above.
examples/alora/alora_server_testing.sh: line 9: --lora-modules: command not found
INFO 05-28 20:41:09 [gpu_model_runner.py:1798] Graph capturing finished in 49 secs, took 1.07 GiB
INFO 05-28 20:41:09 [core.py:161] init engine (profile, create kv cache, warmup model) took 66.84 seconds
INFO 05-28 20:41:09 [core_client.py:442] Core engine process 0 ready.
INFO 05-28 20:41:09 [loggers.py:136] vllm cache_config_info with initialization after num_gpu_blocks is: 21842
INFO 05-28 20:41:09 [api_server.py:1089] Starting vLLM API server on http://0.0.0.0:8000
INFO 05-28 20:41:09 [launcher.py:28] Available routes are:
INFO 05-28 20:41:09 [launcher.py:36] Route: /openapi.json, Methods: GET, HEAD
INFO 05-28 20:41:09 [launcher.py:36] Route: /docs, Methods: GET, HEAD
INFO 05-28 20:41:09 [launcher.py:36] Route: /docs/oauth2-redirect, Methods: GET, HEAD
INFO 05-28 20:41:09 [launcher.py:36] Route: /redoc, Methods: GET, HEAD
INFO 05-28 20:41:09 [launcher.py:36] Route: /health, Methods: GET
INFO 05-28 20:41:09 [launcher.py:36] Route: /load, Methods: GET
INFO 05-28 20:41:09 [launcher.py:36] Route: /ping, Methods: GET, POST
INFO 05-28 20:41:09 [launcher.py:36] Route: /tokenize, Methods: POST
INFO 05-28 20:41:09 [launcher.py:36] Route: /detokenize, Methods: POST
INFO 05-28 20:41:09 [launcher.py:36] Route: /v1/models, Methods: GET
INFO 05-28 20:41:09 [launcher.py:36] Route: /version, Methods: GET
INFO 05-28 20:41:09 [launcher.py:36] Route: /v1/chat/completions, Methods: POST
INFO 05-28 20:41:09 [launcher.py:36] Route: /v1/completions, Methods: POST
INFO 05-28 20:41:09 [launcher.py:36] Route: /v1/embeddings, Methods: POST
INFO 05-28 20:41:09 [launcher.py:36] Route: /pooling, Methods: POST
INFO 05-28 20:41:09 [launcher.py:36] Route: /score, Methods: POST
INFO 05-28 20:41:09 [launcher.py:36] Route: /v1/score, Methods: POST
INFO 05-28 20:41:09 [launcher.py:36] Route: /v1/audio/transcriptions, Methods: POST
INFO 05-28 20:41:09 [launcher.py:36] Route: /rerank, Methods: POST
INFO 05-28 20:41:09 [launcher.py:36] Route: /v1/rerank, Methods: POST
INFO 05-28 20:41:09 [launcher.py:36] Route: /v2/rerank, Methods: POST
INFO 05-28 20:41:09 [launcher.py:36] Route: /invocations, Methods: POST
INFO 05-28 20:41:09 [launcher.py:36] Route: /metrics, Methods: GET
INFO:     Started server process [2518642]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO 05-28 20:42:25 [logger.py:39] Received request cmpl-8aad99c23ca94e1e83e319711a34611a-0: prompt: 'What is MIT?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=600, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [8197, 438, 7131, 49], lora_request: None, prompt_adapter_request: None.
INFO 05-28 20:42:25 [logger.py:39] Received request cmpl-8aad99c23ca94e1e83e319711a34611a-1: prompt: '<|start_of_role|>user<|end_of_role|>What is MIT?<|end_of_text|>\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=600, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [49152, 496, 49153, 8197, 438, 7131, 49, 0, 203], lora_request: None, prompt_adapter_request: None.
INFO 05-28 20:42:25 [async_llm.py:255] Added request cmpl-8aad99c23ca94e1e83e319711a34611a-0.
INFO 05-28 20:42:25 [async_llm.py:255] Added request cmpl-8aad99c23ca94e1e83e319711a34611a-1.
INFO:     127.0.0.1:39090 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:39090 - "POST /v1/completions HTTP/1.1" 404 Not Found
INFO 05-28 20:42:29 [loggers.py:116] Engine 000: Avg prompt throughput: 1.3 tokens/s, Avg generation throughput: 61.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 05-28 20:42:39 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 05-28 20:42:52 [launcher.py:79] Shutting down FastAPI HTTP server.
[rank0]:[W528 20:42:53.322292553 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
examples/alora/alora_server_testing.sh: line 9: --lora-modules: command not found
INFO 05-28 20:44:28 [__init__.py:248] Automatically detected platform cuda.
INFO 05-28 20:44:31 [api_server.py:1042] vLLM API server version 0.1.dev6336+gbe40f32
INFO 05-28 20:44:31 [api_server.py:1043] args: Namespace(subparser='serve', model_tag='ibm-granite/granite-3.2-8b-instruct', config='', host=None, port=8000, uvicorn_log_level='info', disable_uvicorn_access_log=False, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='ibm-granite/granite-3.2-8b-instruct', task='auto', tokenizer=None, tokenizer_mode='auto', trust_remote_code=False, dtype='auto', seed=None, hf_config_path=None, allowed_local_media_path='', revision=None, code_revision=None, rope_scaling={}, rope_theta=None, tokenizer_revision=None, max_model_len=None, quantization=None, enforce_eager=False, max_seq_len_to_capture=8192, max_logprobs=20, disable_sliding_window=False, disable_cascade_attn=False, skip_tokenizer_init=False, enable_prompt_embeds=False, served_model_name=None, disable_async_output_proc=False, config_format='auto', hf_token=None, hf_overrides={}, override_neuron_config={}, override_pooler_config=None, logits_processor_pattern=None, generation_config='auto', override_generation_config={}, enable_sleep_mode=False, model_impl='auto', load_format='auto', download_dir=None, model_loader_extra_config={}, ignore_patterns=None, use_tqdm_on_load=True, qlora_adapter_name_or_path=None, pt_load_map_location='cpu', guided_decoding_backend='auto', guided_decoding_disable_fallback=False, guided_decoding_disable_any_whitespace=False, guided_decoding_disable_additional_properties=False, enable_reasoning=None, reasoning_parser='', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, data_parallel_size=1, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, disable_custom_all_reduce=False, worker_cls='auto', worker_extension_cls='', block_size=None, gpu_memory_utilization=0.9, swap_space=4, kv_cache_dtype='auto', num_gpu_blocks_override=None, enable_prefix_caching=None, prefix_caching_hash_algo='builtin', cpu_offload_gb=0, calculate_kv_scales=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config={}, limit_mm_per_prompt={}, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=True, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=None, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', speculative_config=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, max_num_batched_tokens=None, max_num_seqs=None, max_num_partial_prefills=1, max_long_partial_prefills=1, cuda_graph_sizes=[512], long_prefill_token_threshold=0, num_lookahead_slots=0, scheduler_delay_factor=0.0, preemption_mode=None, num_scheduler_steps=1, multi_step_stream_outputs=True, scheduling_policy='fcfs', enable_chunked_prefill=None, disable_chunked_mm_input=False, scheduler_cls='vllm.core.scheduler.Scheduler', compilation_config=None, kv_transfer_config=None, kv_events_config=None, additional_config=None, use_v2_block_manager=True, disable_log_stats=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False, dispatch_function=<function ServeSubcommand.cmd at 0x14d881074400>)
INFO 05-28 20:44:38 [config.py:752] This model supports multiple tasks: {'score', 'embed', 'reward', 'generate', 'classify'}. Defaulting to 'generate'.
INFO 05-28 20:44:38 [config.py:2057] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 05-28 20:44:42 [__init__.py:248] Automatically detected platform cuda.
INFO 05-28 20:44:43 [core.py:59] Initializing a V1 LLM engine (v0.1.dev6336+gbe40f32) with config: model='ibm-granite/granite-3.2-8b-instruct', speculative_config=None, tokenizer='ibm-granite/granite-3.2-8b-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=ibm-granite/granite-3.2-8b-instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
WARNING 05-28 20:44:44 [utils.py:2621] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x150d44479a00>
INFO 05-28 20:44:44 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 05-28 20:44:44 [cuda.py:221] Using Flash Attention backend on V1 engine.
WARNING 05-28 20:44:44 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
INFO 05-28 20:44:44 [gpu_model_runner.py:1436] Starting to load model ibm-granite/granite-3.2-8b-instruct...
INFO 05-28 20:44:46 [weight_utils.py:257] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:06,  2.25s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:04<00:03,  1.99s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:04<00:01,  1.33s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:06<00:00,  1.51s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:06<00:00,  1.60s/it]

INFO 05-28 20:44:53 [default_loader.py:278] Loading weights took 6.46 seconds
INFO 05-28 20:44:53 [punica_selector.py:18] Using PunicaWrapperGPU.
INFO 05-28 20:44:53 [gpu_model_runner.py:1454] Model loading took 15.3529 GiB and 8.106343 seconds
INFO 05-28 20:45:02 [backends.py:437] Using cache directory: /u/kgreenewald/.cache/vllm/torch_compile_cache/f5e2b8507e/rank_0_0 for vLLM's torch.compile
INFO 05-28 20:45:02 [backends.py:447] Dynamo bytecode transform time: 8.60 s
INFO 05-28 20:45:08 [backends.py:119] Directly load the compiled graph(s) for shape None from the cache, took 5.489 s
INFO 05-28 20:45:09 [monitor.py:33] torch.compile takes 8.60 s in total
INFO 05-28 20:45:10 [kv_cache_utils.py:643] GPU KV cache size: 349,472 tokens
INFO 05-28 20:45:10 [kv_cache_utils.py:646] Maximum concurrency for 131,072 tokens per request: 2.67x
INFO 05-28 20:45:46 [gpu_model_runner.py:1798] Graph capturing finished in 36 secs, took 1.07 GiB
INFO 05-28 20:45:46 [core.py:161] init engine (profile, create kv cache, warmup model) took 52.85 seconds
INFO 05-28 20:45:46 [core_client.py:442] Core engine process 0 ready.
INFO 05-28 20:45:46 [loggers.py:136] vllm cache_config_info with initialization after num_gpu_blocks is: 21842
INFO 05-28 20:45:46 [api_server.py:1089] Starting vLLM API server on http://0.0.0.0:8000
INFO 05-28 20:45:46 [launcher.py:28] Available routes are:
INFO 05-28 20:45:46 [launcher.py:36] Route: /openapi.json, Methods: GET, HEAD
INFO 05-28 20:45:46 [launcher.py:36] Route: /docs, Methods: GET, HEAD
INFO 05-28 20:45:46 [launcher.py:36] Route: /docs/oauth2-redirect, Methods: GET, HEAD
INFO 05-28 20:45:46 [launcher.py:36] Route: /redoc, Methods: GET, HEAD
INFO 05-28 20:45:46 [launcher.py:36] Route: /health, Methods: GET
INFO 05-28 20:45:46 [launcher.py:36] Route: /load, Methods: GET
INFO 05-28 20:45:46 [launcher.py:36] Route: /ping, Methods: GET, POST
INFO 05-28 20:45:46 [launcher.py:36] Route: /tokenize, Methods: POST
INFO 05-28 20:45:46 [launcher.py:36] Route: /detokenize, Methods: POST
INFO 05-28 20:45:46 [launcher.py:36] Route: /v1/models, Methods: GET
INFO 05-28 20:45:46 [launcher.py:36] Route: /version, Methods: GET
INFO 05-28 20:45:46 [launcher.py:36] Route: /v1/chat/completions, Methods: POST
INFO 05-28 20:45:46 [launcher.py:36] Route: /v1/completions, Methods: POST
INFO 05-28 20:45:46 [launcher.py:36] Route: /v1/embeddings, Methods: POST
INFO 05-28 20:45:46 [launcher.py:36] Route: /pooling, Methods: POST
INFO 05-28 20:45:46 [launcher.py:36] Route: /score, Methods: POST
INFO 05-28 20:45:46 [launcher.py:36] Route: /v1/score, Methods: POST
INFO 05-28 20:45:46 [launcher.py:36] Route: /v1/audio/transcriptions, Methods: POST
INFO 05-28 20:45:46 [launcher.py:36] Route: /rerank, Methods: POST
INFO 05-28 20:45:46 [launcher.py:36] Route: /v1/rerank, Methods: POST
INFO 05-28 20:45:46 [launcher.py:36] Route: /v2/rerank, Methods: POST
INFO 05-28 20:45:46 [launcher.py:36] Route: /invocations, Methods: POST
INFO 05-28 20:45:46 [launcher.py:36] Route: /metrics, Methods: GET
INFO:     Started server process [2522877]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO 05-28 20:46:02 [logger.py:39] Received request cmpl-e03db64be7b74d0cb3bb2089db78125e-0: prompt: 'What is MIT?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=600, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [8197, 438, 7131, 49], lora_request: None, prompt_adapter_request: None.
INFO 05-28 20:46:02 [logger.py:39] Received request cmpl-e03db64be7b74d0cb3bb2089db78125e-1: prompt: '<|start_of_role|>user<|end_of_role|>What is MIT?<|end_of_text|>\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=600, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [49152, 496, 49153, 8197, 438, 7131, 49, 0, 203], lora_request: None, prompt_adapter_request: None.
INFO 05-28 20:46:02 [async_llm.py:255] Added request cmpl-e03db64be7b74d0cb3bb2089db78125e-0.
INFO 05-28 20:46:02 [async_llm.py:255] Added request cmpl-e03db64be7b74d0cb3bb2089db78125e-1.
INFO:     127.0.0.1:49926 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:49926 - "POST /v1/completions HTTP/1.1" 404 Not Found
INFO 05-28 20:46:06 [loggers.py:116] Engine 000: Avg prompt throughput: 1.3 tokens/s, Avg generation throughput: 61.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 05-28 20:46:16 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 05-28 20:46:26 [launcher.py:79] Shutting down FastAPI HTTP server.
[rank0]:[W528 20:46:27.446284791 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
examples/alora/alora_server_testing.sh: line 9: --lora-modules: command not found
INFO 05-28 20:47:03 [__init__.py:248] Automatically detected platform cuda.
INFO 05-28 20:47:06 [api_server.py:1042] vLLM API server version 0.1.dev6336+gbe40f32
INFO 05-28 20:47:06 [api_server.py:1043] args: Namespace(subparser='serve', model_tag='ibm-granite/granite-3.2-8b-instruct', config='', host=None, port=8000, uvicorn_log_level='info', disable_uvicorn_access_log=False, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=[LoRAModulePath(name='new_alora', path='/proj/dmfexp/statllm/users/kgreenewald/.cache/huggingface/models/hub/models--ibm-granite--granite-3.2-8b-alora-uncertainty/snapshots/6109ad88201426003e696d023ec67c19e7f3d444', base_model_name='ibm-granite/granite-3.2-8b-instruct')], prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='ibm-granite/granite-3.2-8b-instruct', task='auto', tokenizer=None, tokenizer_mode='auto', trust_remote_code=False, dtype='bfloat16', seed=None, hf_config_path=None, allowed_local_media_path='', revision=None, code_revision=None, rope_scaling={}, rope_theta=None, tokenizer_revision=None, max_model_len=None, quantization=None, enforce_eager=False, max_seq_len_to_capture=8192, max_logprobs=20, disable_sliding_window=False, disable_cascade_attn=False, skip_tokenizer_init=False, enable_prompt_embeds=False, served_model_name=None, disable_async_output_proc=False, config_format='auto', hf_token=None, hf_overrides={}, override_neuron_config={}, override_pooler_config=None, logits_processor_pattern=None, generation_config='auto', override_generation_config={}, enable_sleep_mode=False, model_impl='auto', load_format='auto', download_dir=None, model_loader_extra_config={}, ignore_patterns=None, use_tqdm_on_load=True, qlora_adapter_name_or_path=None, pt_load_map_location='cpu', guided_decoding_backend='auto', guided_decoding_disable_fallback=False, guided_decoding_disable_any_whitespace=False, guided_decoding_disable_additional_properties=False, enable_reasoning=None, reasoning_parser='', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, data_parallel_size=1, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, disable_custom_all_reduce=False, worker_cls='auto', worker_extension_cls='', block_size=None, gpu_memory_utilization=0.9, swap_space=4, kv_cache_dtype='auto', num_gpu_blocks_override=None, enable_prefix_caching=False, prefix_caching_hash_algo='builtin', cpu_offload_gb=0, calculate_kv_scales=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config={}, limit_mm_per_prompt={}, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=True, enable_lora_bias=False, max_loras=1, max_lora_rank=64, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=None, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', speculative_config=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, max_num_batched_tokens=None, max_num_seqs=None, max_num_partial_prefills=1, max_long_partial_prefills=1, cuda_graph_sizes=[512], long_prefill_token_threshold=0, num_lookahead_slots=0, scheduler_delay_factor=0.0, preemption_mode=None, num_scheduler_steps=1, multi_step_stream_outputs=True, scheduling_policy='fcfs', enable_chunked_prefill=None, disable_chunked_mm_input=False, scheduler_cls='vllm.core.scheduler.Scheduler', compilation_config=None, kv_transfer_config=None, kv_events_config=None, additional_config=None, use_v2_block_manager=True, disable_log_stats=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False, dispatch_function=<function ServeSubcommand.cmd at 0x151f48748400>)
INFO 05-28 20:47:13 [config.py:752] This model supports multiple tasks: {'embed', 'classify', 'score', 'generate', 'reward'}. Defaulting to 'generate'.
INFO 05-28 20:47:13 [config.py:2057] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 05-28 20:47:16 [__init__.py:248] Automatically detected platform cuda.
INFO 05-28 20:47:19 [core.py:59] Initializing a V1 LLM engine (v0.1.dev6336+gbe40f32) with config: model='ibm-granite/granite-3.2-8b-instruct', speculative_config=None, tokenizer='ibm-granite/granite-3.2-8b-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=ibm-granite/granite-3.2-8b-instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
WARNING 05-28 20:47:19 [utils.py:2621] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x153b1e8fe3c0>
INFO 05-28 20:47:20 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 05-28 20:47:20 [cuda.py:221] Using Flash Attention backend on V1 engine.
WARNING 05-28 20:47:20 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
INFO 05-28 20:47:20 [gpu_model_runner.py:1436] Starting to load model ibm-granite/granite-3.2-8b-instruct...
INFO 05-28 20:47:21 [weight_utils.py:257] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:05,  1.75s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:03<00:03,  1.79s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:04<00:01,  1.22s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:05<00:00,  1.44s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:05<00:00,  1.47s/it]

INFO 05-28 20:47:27 [default_loader.py:278] Loading weights took 5.98 seconds
INFO 05-28 20:47:27 [punica_selector.py:18] Using PunicaWrapperGPU.
INFO 05-28 20:47:27 [gpu_model_runner.py:1454] Model loading took 15.6392 GiB and 6.560143 seconds
INFO 05-28 20:47:36 [backends.py:437] Using cache directory: /u/kgreenewald/.cache/vllm/torch_compile_cache/23e2141a42/rank_0_0 for vLLM's torch.compile
INFO 05-28 20:47:36 [backends.py:447] Dynamo bytecode transform time: 8.73 s
INFO 05-28 20:47:41 [backends.py:138] Cache the graph of shape None for later use
INFO 05-28 20:48:14 [backends.py:150] Compiling a graph for general shape takes 37.22 s
INFO 05-28 20:48:38 [monitor.py:33] torch.compile takes 45.95 s in total
INFO 05-28 20:48:39 [kv_cache_utils.py:643] GPU KV cache size: 346,272 tokens
INFO 05-28 20:48:39 [kv_cache_utils.py:646] Maximum concurrency for 131,072 tokens per request: 2.64x
INFO 05-28 20:49:30 [gpu_model_runner.py:1798] Graph capturing finished in 51 secs, took 1.07 GiB
INFO 05-28 20:49:30 [core.py:161] init engine (profile, create kv cache, warmup model) took 122.53 seconds
INFO 05-28 20:49:30 [core_client.py:442] Core engine process 0 ready.
INFO 05-28 20:49:30 [loggers.py:136] vllm cache_config_info with initialization after num_gpu_blocks is: 21642
INFO 05-28 20:49:30 [serving_models.py:185] Loaded new LoRA adapter: name 'new_alora', path '/proj/dmfexp/statllm/users/kgreenewald/.cache/huggingface/models/hub/models--ibm-granite--granite-3.2-8b-alora-uncertainty/snapshots/6109ad88201426003e696d023ec67c19e7f3d444'
INFO 05-28 20:49:30 [api_server.py:1089] Starting vLLM API server on http://0.0.0.0:8000
INFO 05-28 20:49:30 [launcher.py:28] Available routes are:
INFO 05-28 20:49:30 [launcher.py:36] Route: /openapi.json, Methods: GET, HEAD
INFO 05-28 20:49:30 [launcher.py:36] Route: /docs, Methods: GET, HEAD
INFO 05-28 20:49:30 [launcher.py:36] Route: /docs/oauth2-redirect, Methods: GET, HEAD
INFO 05-28 20:49:30 [launcher.py:36] Route: /redoc, Methods: GET, HEAD
INFO 05-28 20:49:30 [launcher.py:36] Route: /health, Methods: GET
INFO 05-28 20:49:30 [launcher.py:36] Route: /load, Methods: GET
INFO 05-28 20:49:30 [launcher.py:36] Route: /ping, Methods: GET, POST
INFO 05-28 20:49:30 [launcher.py:36] Route: /tokenize, Methods: POST
INFO 05-28 20:49:30 [launcher.py:36] Route: /detokenize, Methods: POST
INFO 05-28 20:49:30 [launcher.py:36] Route: /v1/models, Methods: GET
INFO 05-28 20:49:30 [launcher.py:36] Route: /version, Methods: GET
INFO 05-28 20:49:30 [launcher.py:36] Route: /v1/chat/completions, Methods: POST
INFO 05-28 20:49:30 [launcher.py:36] Route: /v1/completions, Methods: POST
INFO 05-28 20:49:30 [launcher.py:36] Route: /v1/embeddings, Methods: POST
INFO 05-28 20:49:30 [launcher.py:36] Route: /pooling, Methods: POST
INFO 05-28 20:49:30 [launcher.py:36] Route: /score, Methods: POST
INFO 05-28 20:49:30 [launcher.py:36] Route: /v1/score, Methods: POST
INFO 05-28 20:49:30 [launcher.py:36] Route: /v1/audio/transcriptions, Methods: POST
INFO 05-28 20:49:30 [launcher.py:36] Route: /rerank, Methods: POST
INFO 05-28 20:49:30 [launcher.py:36] Route: /v1/rerank, Methods: POST
INFO 05-28 20:49:30 [launcher.py:36] Route: /v2/rerank, Methods: POST
INFO 05-28 20:49:30 [launcher.py:36] Route: /invocations, Methods: POST
INFO 05-28 20:49:30 [launcher.py:36] Route: /metrics, Methods: GET
INFO:     Started server process [2525283]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO 05-28 20:50:03 [logger.py:39] Received request cmpl-682c20d1268644b9be8d7572c852f253-0: prompt: 'What is MIT?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=600, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [8197, 438, 7131, 49], lora_request: None, prompt_adapter_request: None.
INFO 05-28 20:50:03 [logger.py:39] Received request cmpl-682c20d1268644b9be8d7572c852f253-1: prompt: '<|start_of_role|>user<|end_of_role|>What is MIT?<|end_of_text|>\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=600, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [49152, 496, 49153, 8197, 438, 7131, 49, 0, 203], lora_request: None, prompt_adapter_request: None.
INFO 05-28 20:50:03 [async_llm.py:255] Added request cmpl-682c20d1268644b9be8d7572c852f253-0.
INFO 05-28 20:50:03 [async_llm.py:255] Added request cmpl-682c20d1268644b9be8d7572c852f253-1.
INFO:     127.0.0.1:45734 - "POST /v1/completions HTTP/1.1" 200 OK
WARNING 05-28 20:50:06 [tokenizer.py:294] No tokenizer found in /proj/dmfexp/statllm/users/kgreenewald/.cache/huggingface/models/hub/models--ibm-granite--granite-3.2-8b-alora-uncertainty/snapshots/6109ad88201426003e696d023ec67c19e7f3d444, using base model tokenizer instead. (Exception: <class 'transformers.models.granite.configuration_granite.GraniteConfig'>)
INFO 05-28 20:50:06 [logger.py:39] Received request cmpl-16fa8e44932d468e85b1b6c8a418a202-0: prompt: "What is MIT?\n\nMIT, or the Massachusetts Institute of Technology, is a prestigious private research university located in Cambridge, Massachusetts, United States. It is renowned for its strong emphasis on scientific, technological, and engineering education and research.\n\nMIT was founded in 1861 and is consistently ranked among the top universities globally. The institution is known for its rigorous academic programs, innovative research, and significant contributions to various fields, including computer science, engineering, physics, mathematics, and economics.\n\nMIT's campus is home to numerous research centers, laboratories, and institutes, which facilitate groundbreaking discoveries and advancements. The university is also recognized for its commitment to fostering a diverse and inclusive community, as well as its dedication to addressing global challenges through interdisciplinary collaboration.\n\nMIT's alumni and faculty have made significant impacts in various industries, including technology, academia, and government. Some notable alumni include physicist Richard Feynman, computer scientist Marvin Minsky, and entrepreneur Elon Musk.\n\nIn summary, MIT is a world-class research university with a strong focus on science, technology, and engineering. Its commitment to innovation, research, and education has solidified its reputation as a leading institution in the global academic landscape.<|end_of_text|>\n<|start_of_role|>certainty<|end_of_role|>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [8197, 438, 7131, 49, 203, 203, 6675, 30, 556, 322, 28051, 867, 43017, 1421, 27358, 432, 21909, 30, 438, 312, 41804, 365, 2406, 945, 13234, 707, 9190, 15282, 328, 31719, 12154, 30, 28051, 867, 43017, 1421, 30, 28882, 18600, 32, 2030, 438, 316, 2481, 318, 436, 2819, 12101, 12480, 22557, 544, 31722, 30, 7341, 1347, 11081, 30, 461, 33627, 31741, 461, 13234, 32, 203, 203, 6675, 1597, 16870, 6918, 328, 225, 35, 42, 40, 35, 461, 438, 45631, 11263, 318, 17660, 322, 2663, 707, 5810, 2105, 29585, 32, 886, 31681, 438, 8967, 436, 2819, 34315, 271, 3291, 312, 31505, 18335, 30, 328, 15007, 1353, 13234, 30, 461, 14724, 25198, 372, 10297, 3829, 30, 6237, 10670, 27536, 30, 33627, 30, 28231, 30, 32775, 101, 30, 461, 32848, 1316, 32, 203, 203, 6675, 1182, 16303, 352, 438, 6765, 372, 21639, 352, 13234, 40962, 30, 20173, 717, 732, 30, 461, 1440, 283, 3676, 30, 1510, 44288, 16398, 20806, 23149, 732, 461, 18514, 1728, 32, 886, 707, 9190, 438, 2329, 27149, 436, 2819, 4583, 469, 372, 296, 649, 8637, 312, 4451, 24026, 461, 35216, 11896, 30, 619, 4487, 619, 2819, 20405, 367, 372, 43383, 3649, 35439, 3919, 1426, 1285, 22257, 3172, 39973, 32, 203, 203, 6675, 1182, 743, 1129, 91, 461, 9652, 31146, 1159, 5590, 14724, 14932, 101, 328, 10297, 4842, 675, 3064, 30, 6237, 21519, 30, 312, 9002, 95, 905, 30, 461, 31789, 32, 4185, 646, 444, 743, 1129, 91, 2305, 26571, 295, 427, 39042, 15311, 841, 1588, 30, 10670, 2197, 1606, 427, 8359, 13762, 7507, 14265, 30, 461, 17817, 1001, 941, 305, 516, 9512, 488, 352, 93, 32, 203, 203, 383, 8822, 30, 7131, 438, 312, 5788, 31, 823, 13234, 707, 9190, 623, 312, 12101, 9280, 544, 27536, 30, 21519, 30, 461, 33627, 32, 26983, 4583, 469, 372, 328, 38795, 30, 13234, 30, 461, 31741, 1401, 4810, 1639, 2819, 316, 45783, 619, 312, 15996, 31681, 328, 322, 3649, 312, 31505, 37009, 32, 0, 203, 49152, 6989, 24933, 49153], lora_request: LoRARequest(lora_name='new_alora', lora_int_id=1, lora_path='/proj/dmfexp/statllm/users/kgreenewald/.cache/huggingface/models/hub/models--ibm-granite--granite-3.2-8b-alora-uncertainty/snapshots/6109ad88201426003e696d023ec67c19e7f3d444', lora_local_path=None, long_lora_max_len=None, base_model_name='ibm-granite/granite-3.2-8b-instruct'), prompt_adapter_request: None.
INFO 05-28 20:50:06 [logger.py:39] Received request cmpl-16fa8e44932d468e85b1b6c8a418a202-1: prompt: "<|start_of_role|>user<|end_of_role|>What is MIT?<|end_of_text|>\n\nMIT, or the Massachusetts Institute of Technology, is a prestigious private research university located in Cambridge, Massachusetts, United States. Established in 1861, MIT is renowned for its commitment to advancing knowledge and education in science, technology, engineering, arts, and humanities. It is consistently ranked among the top universities globally for its academic rigor, research output, and innovation.\n\nMIT is known for its strong emphasis on undergraduate education, offering a wide range of disciplines and fostering a collaborative learning environment. The institute is home to numerous research centers and laboratories, where faculty and students work together on cutting-edge projects in various fields, such as artificial intelligence, robotics, materials science, and environmental sustainability.\n\nThe university's campus is a vibrant hub of innovation, with state-of-the-art facilities and resources dedicated to fostering creativity and intellectual curiosity. MIT's alumni include numerous influential figures in technology, science, and business, contributing to its reputation as a global leader in higher education and research.\n\nIf you have any specific questions about MIT, its programs, research, or notable achievements, feel free to ask! I'm here to help.<|end_of_text|>\n<|start_of_role|>certainty<|end_of_role|>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [49152, 496, 49153, 8197, 438, 7131, 49, 0, 203, 203, 6675, 30, 556, 322, 28051, 867, 43017, 1421, 27358, 432, 21909, 30, 438, 312, 41804, 365, 2406, 945, 13234, 707, 9190, 15282, 328, 31719, 12154, 30, 28051, 867, 43017, 1421, 30, 28882, 18600, 32, 14164, 360, 5829, 328, 225, 35, 42, 40, 35, 30, 7131, 438, 316, 2481, 318, 436, 2819, 4583, 469, 372, 9767, 18522, 16969, 461, 31741, 328, 27536, 30, 21519, 30, 33627, 30, 5549, 101, 30, 461, 13462, 2105, 32, 2030, 438, 45631, 11263, 318, 17660, 322, 2663, 707, 5810, 2105, 29585, 436, 2819, 312, 31505, 34315, 271, 30, 13234, 1688, 30, 461, 328, 38795, 32, 203, 203, 6675, 438, 8967, 436, 2819, 12101, 12480, 22557, 544, 1531, 4943, 27415, 31741, 30, 45525, 312, 16264, 2155, 432, 1214, 22257, 2353, 461, 296, 649, 8637, 312, 21368, 1353, 9608, 4706, 32, 886, 1440, 11461, 438, 6765, 372, 21639, 352, 13234, 40962, 461, 20173, 717, 732, 30, 2154, 9652, 31146, 461, 16512, 1389, 10752, 544, 11909, 1054, 31, 5854, 8528, 328, 10297, 3829, 30, 3751, 619, 5549, 31251, 629, 21488, 30, 13047, 1316, 30, 12953, 27536, 30, 461, 4706, 279, 309, 35170, 3452, 32, 203, 203, 1318, 707, 9190, 1182, 16303, 352, 438, 312, 8943, 839, 692, 20089, 432, 328, 38795, 30, 623, 1603, 31, 1028, 31, 1382, 31, 502, 43798, 461, 6086, 23112, 372, 296, 649, 8637, 2020, 43826, 461, 31067, 43119, 2738, 3945, 543, 32, 7131, 1182, 743, 1129, 91, 2305, 21639, 352, 43229, 2722, 37526, 328, 21519, 30, 27536, 30, 461, 12590, 30, 23476, 372, 2819, 316, 45783, 619, 312, 3649, 22021, 328, 12224, 31741, 461, 13234, 32, 203, 203, 2797, 844, 1159, 1346, 2818, 10017, 2625, 7131, 30, 2819, 18335, 30, 13234, 30, 556, 646, 444, 14002, 42695, 30, 10871, 3741, 372, 7660, 19, 439, 3464, 2442, 372, 3049, 32, 0, 203, 49152, 6989, 24933, 49153], lora_request: LoRARequest(lora_name='new_alora', lora_int_id=1, lora_path='/proj/dmfexp/statllm/users/kgreenewald/.cache/huggingface/models/hub/models--ibm-granite--granite-3.2-8b-alora-uncertainty/snapshots/6109ad88201426003e696d023ec67c19e7f3d444', lora_local_path=None, long_lora_max_len=None, base_model_name='ibm-granite/granite-3.2-8b-instruct'), prompt_adapter_request: None.
INFO 05-28 20:50:06 [async_llm.py:255] Added request cmpl-16fa8e44932d468e85b1b6c8a418a202-0.
INFO 05-28 20:50:06 [async_llm.py:255] Added request cmpl-16fa8e44932d468e85b1b6c8a418a202-1.
INFO:     127.0.0.1:45734 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-28 20:50:10 [loggers.py:116] Engine 000: Avg prompt throughput: 65.0 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 05-28 20:50:20 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 05-28 20:53:44 [launcher.py:79] Shutting down FastAPI HTTP server.
[rank0]:[W528 20:53:45.603328814 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
INFO 06-03 21:05:03 [__init__.py:248] Automatically detected platform cuda.
INFO 06-03 21:05:06 [api_server.py:1042] vLLM API server version 0.1.dev6336+gbe40f32
INFO 06-03 21:05:06 [api_server.py:1043] args: Namespace(subparser='serve', model_tag='ibm-granite/granite-3.2-8b-instruct', config='', host=None, port=8000, uvicorn_log_level='info', disable_uvicorn_access_log=False, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=[LoRAModulePath(name='new_alora', path='/proj/dmfexp/statllm/users/kgreenewald/.cache/huggingface/models/hub/models--ibm-granite--granite-3.2-8b-alora-uncertainty/snapshots/6109ad88201426003e696d023ec67c19e7f3d444', base_model_name='ibm-granite/granite-3.2-8b-instruct')], prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='ibm-granite/granite-3.2-8b-instruct', task='auto', tokenizer=None, tokenizer_mode='auto', trust_remote_code=False, dtype='bfloat16', seed=None, hf_config_path=None, allowed_local_media_path='', revision=None, code_revision=None, rope_scaling={}, rope_theta=None, tokenizer_revision=None, max_model_len=None, quantization=None, enforce_eager=False, max_seq_len_to_capture=8192, max_logprobs=20, disable_sliding_window=False, disable_cascade_attn=False, skip_tokenizer_init=False, enable_prompt_embeds=False, served_model_name=None, disable_async_output_proc=False, config_format='auto', hf_token=None, hf_overrides={}, override_neuron_config={}, override_pooler_config=None, logits_processor_pattern=None, generation_config='auto', override_generation_config={}, enable_sleep_mode=False, model_impl='auto', load_format='auto', download_dir=None, model_loader_extra_config={}, ignore_patterns=None, use_tqdm_on_load=True, qlora_adapter_name_or_path=None, pt_load_map_location='cpu', guided_decoding_backend='auto', guided_decoding_disable_fallback=False, guided_decoding_disable_any_whitespace=False, guided_decoding_disable_additional_properties=False, enable_reasoning=None, reasoning_parser='', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, data_parallel_size=1, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, disable_custom_all_reduce=False, worker_cls='auto', worker_extension_cls='', block_size=None, gpu_memory_utilization=0.9, swap_space=4, kv_cache_dtype='auto', num_gpu_blocks_override=None, enable_prefix_caching=False, prefix_caching_hash_algo='builtin', cpu_offload_gb=0, calculate_kv_scales=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config={}, limit_mm_per_prompt={}, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=True, enable_lora_bias=False, max_loras=1, max_lora_rank=64, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=None, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', speculative_config=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, max_num_batched_tokens=None, max_num_seqs=None, max_num_partial_prefills=1, max_long_partial_prefills=1, cuda_graph_sizes=[512], long_prefill_token_threshold=0, num_lookahead_slots=0, scheduler_delay_factor=0.0, preemption_mode=None, num_scheduler_steps=1, multi_step_stream_outputs=True, scheduling_policy='fcfs', enable_chunked_prefill=None, disable_chunked_mm_input=False, scheduler_cls='vllm.core.scheduler.Scheduler', compilation_config=None, kv_transfer_config=None, kv_events_config=None, additional_config=None, use_v2_block_manager=True, disable_log_stats=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False, dispatch_function=<function ServeSubcommand.cmd at 0x147f6212c9a0>)
INFO 06-03 21:05:13 [config.py:752] This model supports multiple tasks: {'classify', 'score', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 06-03 21:05:13 [config.py:2057] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 06-03 21:05:16 [__init__.py:248] Automatically detected platform cuda.
INFO 06-03 21:05:18 [core.py:59] Initializing a V1 LLM engine (v0.1.dev6336+gbe40f32) with config: model='ibm-granite/granite-3.2-8b-instruct', speculative_config=None, tokenizer='ibm-granite/granite-3.2-8b-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=ibm-granite/granite-3.2-8b-instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
WARNING 06-03 21:05:19 [utils.py:2621] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x14cdf4173b90>
INFO 06-03 21:05:20 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 06-03 21:05:20 [cuda.py:221] Using Flash Attention backend on V1 engine.
WARNING 06-03 21:05:20 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
INFO 06-03 21:05:20 [gpu_model_runner.py:1434] Starting to load model ibm-granite/granite-3.2-8b-instruct...
INFO 06-03 21:05:21 [weight_utils.py:257] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:06,  2.09s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:04<00:04,  2.17s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:05<00:01,  1.52s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:07<00:00,  1.72s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:07<00:00,  1.77s/it]

INFO 06-03 21:05:28 [default_loader.py:278] Loading weights took 7.22 seconds
INFO 06-03 21:05:28 [punica_selector.py:18] Using PunicaWrapperGPU.
INFO 06-03 21:05:28 [gpu_model_runner.py:1452] Model loading took 15.6392 GiB and 7.903709 seconds
INFO 06-03 21:05:40 [backends.py:437] Using cache directory: /u/kgreenewald/.cache/vllm/torch_compile_cache/32112be5c2/rank_0_0 for vLLM's torch.compile
INFO 06-03 21:05:40 [backends.py:447] Dynamo bytecode transform time: 10.29 s
INFO 06-03 21:05:44 [backends.py:138] Cache the graph of shape None for later use
INFO 06-03 21:06:22 [backends.py:150] Compiling a graph for general shape takes 40.21 s
INFO 06-03 21:06:47 [monitor.py:33] torch.compile takes 50.50 s in total
INFO 06-03 21:06:47 [kv_cache_utils.py:643] GPU KV cache size: 346,272 tokens
INFO 06-03 21:06:47 [kv_cache_utils.py:646] Maximum concurrency for 131,072 tokens per request: 2.64x
[rank0]:[W603 21:09:41.450351602 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/proj/dmfexp/statllm/share/envs/IBMaloraVLLM/bin/vllm", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/proj/dmfexp/statllm/users/kgreenewald/Thermometer/vllm-fork/vllm/vllm/entrypoints/cli/main.py", line 53, in main
    args.dispatch_function(args)
  File "/proj/dmfexp/statllm/users/kgreenewald/Thermometer/vllm-fork/vllm/vllm/entrypoints/cli/serve.py", line 27, in cmd
    uvloop.run(run_server(args))
  File "/proj/dmfexp/statllm/share/envs/IBMaloraVLLM/lib/python3.12/site-packages/uvloop/__init__.py", line 109, in run
    return __asyncio.run(
           ^^^^^^^^^^^^^^
  File "/proj/dmfexp/statllm/share/envs/IBMaloraVLLM/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/proj/dmfexp/statllm/share/envs/IBMaloraVLLM/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "uvloop/loop.pyx", line 1512, in uvloop.loop.Loop.run_until_complete
  File "uvloop/loop.pyx", line 1505, in uvloop.loop.Loop.run_until_complete
  File "uvloop/loop.pyx", line 1379, in uvloop.loop.Loop.run_forever
  File "uvloop/loop.pyx", line 557, in uvloop.loop.Loop._run
  File "uvloop/loop.pyx", line 476, in uvloop.loop.Loop._on_idle
  File "uvloop/cbhandles.pyx", line 83, in uvloop.loop.Handle._run
  File "uvloop/cbhandles.pyx", line 61, in uvloop.loop.Handle._run
  File "/proj/dmfexp/statllm/share/envs/IBMaloraVLLM/lib/python3.12/site-packages/uvloop/__init__.py", line 61, in wrapper
    return await main
           ^^^^^^^^^^
  File "/proj/dmfexp/statllm/users/kgreenewald/Thermometer/vllm-fork/vllm/vllm/entrypoints/openai/api_server.py", line 1077, in run_server
    async with build_async_engine_client(args) as engine_client:
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/dmfexp/statllm/share/envs/IBMaloraVLLM/lib/python3.12/contextlib.py", line 210, in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/proj/dmfexp/statllm/users/kgreenewald/Thermometer/vllm-fork/vllm/vllm/entrypoints/openai/api_server.py", line 146, in build_async_engine_client
    async with build_async_engine_client_from_engine_args(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/dmfexp/statllm/share/envs/IBMaloraVLLM/lib/python3.12/contextlib.py", line 210, in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/proj/dmfexp/statllm/users/kgreenewald/Thermometer/vllm-fork/vllm/vllm/entrypoints/openai/api_server.py", line 178, in build_async_engine_client_from_engine_args
    async_llm = AsyncLLM.from_vllm_config(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/dmfexp/statllm/users/kgreenewald/Thermometer/vllm-fork/vllm/vllm/v1/engine/async_llm.py", line 151, in from_vllm_config
    return cls(
           ^^^^
  File "/proj/dmfexp/statllm/users/kgreenewald/Thermometer/vllm-fork/vllm/vllm/v1/engine/async_llm.py", line 118, in __init__
    self.engine_core = core_client_class(
                       ^^^^^^^^^^^^^^^^^^
  File "/proj/dmfexp/statllm/users/kgreenewald/Thermometer/vllm-fork/vllm/vllm/v1/engine/core_client.py", line 649, in __init__
    super().__init__(
  File "/proj/dmfexp/statllm/users/kgreenewald/Thermometer/vllm-fork/vllm/vllm/v1/engine/core_client.py", line 400, in __init__
    self._wait_for_engine_startup()
  File "/proj/dmfexp/statllm/users/kgreenewald/Thermometer/vllm-fork/vllm/vllm/v1/engine/core_client.py", line 425, in _wait_for_engine_startup
    events = poller.poll(STARTUP_POLL_PERIOD_MS)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/dmfexp/statllm/share/envs/IBMaloraVLLM/lib/python3.12/site-packages/zmq/sugar/poll.py", line 106, in poll
    return zmq_poll(self.sockets, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "_zmq.py", line 1609, in zmq.backend.cython._zmq.zmq_poll
  File "_zmq.py", line 169, in zmq.backend.cython._zmq._check_rc
  File "/proj/dmfexp/statllm/users/kgreenewald/Thermometer/vllm-fork/vllm/vllm/entrypoints/openai/api_server.py", line 1073, in signal_handler
    raise KeyboardInterrupt("terminated")
KeyboardInterrupt: terminated
INFO 06-03 21:12:53 [__init__.py:248] Automatically detected platform cuda.
INFO 06-03 21:12:56 [api_server.py:1042] vLLM API server version 0.1.dev6336+gbe40f32
INFO 06-03 21:12:56 [api_server.py:1043] args: Namespace(subparser='serve', model_tag='ibm-granite/granite-3.2-8b-instruct', config='', host=None, port=8000, uvicorn_log_level='info', disable_uvicorn_access_log=False, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=[LoRAModulePath(name='new_alora', path='/proj/dmfexp/statllm/users/kgreenewald/.cache/huggingface/models/hub/models--ibm-granite--granite-3.2-8b-alora-uncertainty/snapshots/6109ad88201426003e696d023ec67c19e7f3d444', base_model_name='ibm-granite/granite-3.2-8b-instruct')], prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='ibm-granite/granite-3.2-8b-instruct', task='auto', tokenizer=None, tokenizer_mode='auto', trust_remote_code=False, dtype='bfloat16', seed=None, hf_config_path=None, allowed_local_media_path='', revision=None, code_revision=None, rope_scaling={}, rope_theta=None, tokenizer_revision=None, max_model_len=None, quantization=None, enforce_eager=False, max_seq_len_to_capture=8192, max_logprobs=20, disable_sliding_window=False, disable_cascade_attn=False, skip_tokenizer_init=False, enable_prompt_embeds=False, served_model_name=None, disable_async_output_proc=False, config_format='auto', hf_token=None, hf_overrides={}, override_neuron_config={}, override_pooler_config=None, logits_processor_pattern=None, generation_config='auto', override_generation_config={}, enable_sleep_mode=False, model_impl='auto', load_format='auto', download_dir=None, model_loader_extra_config={}, ignore_patterns=None, use_tqdm_on_load=True, qlora_adapter_name_or_path=None, pt_load_map_location='cpu', guided_decoding_backend='auto', guided_decoding_disable_fallback=False, guided_decoding_disable_any_whitespace=False, guided_decoding_disable_additional_properties=False, enable_reasoning=None, reasoning_parser='', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, data_parallel_size=1, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, disable_custom_all_reduce=False, worker_cls='auto', worker_extension_cls='', block_size=None, gpu_memory_utilization=0.9, swap_space=4, kv_cache_dtype='auto', num_gpu_blocks_override=None, enable_prefix_caching=True, prefix_caching_hash_algo='builtin', cpu_offload_gb=0, calculate_kv_scales=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config={}, limit_mm_per_prompt={}, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=True, enable_lora_bias=False, max_loras=1, max_lora_rank=64, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=None, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', speculative_config=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, max_num_batched_tokens=None, max_num_seqs=None, max_num_partial_prefills=1, max_long_partial_prefills=1, cuda_graph_sizes=[512], long_prefill_token_threshold=0, num_lookahead_slots=0, scheduler_delay_factor=0.0, preemption_mode=None, num_scheduler_steps=1, multi_step_stream_outputs=True, scheduling_policy='fcfs', enable_chunked_prefill=None, disable_chunked_mm_input=False, scheduler_cls='vllm.core.scheduler.Scheduler', compilation_config=None, kv_transfer_config=None, kv_events_config=None, additional_config=None, use_v2_block_manager=True, disable_log_stats=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False, dispatch_function=<function ServeSubcommand.cmd at 0x148ccbc6c5e0>)
INFO 06-03 21:13:03 [config.py:752] This model supports multiple tasks: {'generate', 'embed', 'reward', 'score', 'classify'}. Defaulting to 'generate'.
INFO 06-03 21:13:03 [config.py:2057] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 06-03 21:13:06 [__init__.py:248] Automatically detected platform cuda.
INFO 06-03 21:13:08 [core.py:59] Initializing a V1 LLM engine (v0.1.dev6336+gbe40f32) with config: model='ibm-granite/granite-3.2-8b-instruct', speculative_config=None, tokenizer='ibm-granite/granite-3.2-8b-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=ibm-granite/granite-3.2-8b-instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
WARNING 06-03 21:13:09 [utils.py:2621] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x14c0fda0bc50>
INFO 06-03 21:13:10 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 06-03 21:13:10 [cuda.py:221] Using Flash Attention backend on V1 engine.
WARNING 06-03 21:13:10 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
INFO 06-03 21:13:10 [gpu_model_runner.py:1434] Starting to load model ibm-granite/granite-3.2-8b-instruct...
INFO 06-03 21:13:10 [weight_utils.py:257] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:05,  2.00s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:04<00:04,  2.09s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:04<00:01,  1.46s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:06<00:00,  1.65s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:06<00:00,  1.70s/it]

INFO 06-03 21:13:17 [default_loader.py:278] Loading weights took 6.96 seconds
INFO 06-03 21:13:17 [punica_selector.py:18] Using PunicaWrapperGPU.
INFO 06-03 21:13:17 [gpu_model_runner.py:1452] Model loading took 15.6392 GiB and 7.565739 seconds
INFO 06-03 21:13:30 [backends.py:437] Using cache directory: /u/kgreenewald/.cache/vllm/torch_compile_cache/32112be5c2/rank_0_0 for vLLM's torch.compile
INFO 06-03 21:13:30 [backends.py:447] Dynamo bytecode transform time: 10.31 s
INFO 06-03 21:13:38 [backends.py:119] Directly load the compiled graph(s) for shape None from the cache, took 6.635 s
INFO 06-03 21:13:39 [monitor.py:33] torch.compile takes 10.31 s in total
INFO 06-03 21:13:40 [kv_cache_utils.py:643] GPU KV cache size: 346,384 tokens
INFO 06-03 21:13:40 [kv_cache_utils.py:646] Maximum concurrency for 131,072 tokens per request: 2.64x
INFO 06-03 21:18:11 [gpu_model_runner.py:1796] Graph capturing finished in 271 secs, took 1.07 GiB
INFO 06-03 21:18:13 [core.py:161] init engine (profile, create kv cache, warmup model) took 295.65 seconds
INFO 06-03 21:18:13 [core_client.py:442] Core engine process 0 ready.
INFO 06-03 21:18:13 [loggers.py:136] vllm cache_config_info with initialization after num_gpu_blocks is: 21649
INFO 06-03 21:18:16 [serving_models.py:185] Loaded new LoRA adapter: name 'new_alora', path '/proj/dmfexp/statllm/users/kgreenewald/.cache/huggingface/models/hub/models--ibm-granite--granite-3.2-8b-alora-uncertainty/snapshots/6109ad88201426003e696d023ec67c19e7f3d444'
INFO 06-03 21:18:16 [api_server.py:1089] Starting vLLM API server on http://0.0.0.0:8000
INFO 06-03 21:18:16 [launcher.py:28] Available routes are:
INFO 06-03 21:18:16 [launcher.py:36] Route: /openapi.json, Methods: HEAD, GET
INFO 06-03 21:18:16 [launcher.py:36] Route: /docs, Methods: HEAD, GET
INFO 06-03 21:18:16 [launcher.py:36] Route: /docs/oauth2-redirect, Methods: HEAD, GET
INFO 06-03 21:18:16 [launcher.py:36] Route: /redoc, Methods: HEAD, GET
INFO 06-03 21:18:16 [launcher.py:36] Route: /health, Methods: GET
INFO 06-03 21:18:16 [launcher.py:36] Route: /load, Methods: GET
INFO 06-03 21:18:16 [launcher.py:36] Route: /ping, Methods: GET, POST
INFO 06-03 21:18:16 [launcher.py:36] Route: /tokenize, Methods: POST
INFO 06-03 21:18:16 [launcher.py:36] Route: /detokenize, Methods: POST
INFO 06-03 21:18:16 [launcher.py:36] Route: /v1/models, Methods: GET
INFO 06-03 21:18:16 [launcher.py:36] Route: /version, Methods: GET
INFO 06-03 21:18:16 [launcher.py:36] Route: /v1/chat/completions, Methods: POST
INFO 06-03 21:18:16 [launcher.py:36] Route: /v1/completions, Methods: POST
INFO 06-03 21:18:16 [launcher.py:36] Route: /v1/embeddings, Methods: POST
INFO 06-03 21:18:16 [launcher.py:36] Route: /pooling, Methods: POST
INFO 06-03 21:18:16 [launcher.py:36] Route: /score, Methods: POST
INFO 06-03 21:18:16 [launcher.py:36] Route: /v1/score, Methods: POST
INFO 06-03 21:18:16 [launcher.py:36] Route: /v1/audio/transcriptions, Methods: POST
INFO 06-03 21:18:16 [launcher.py:36] Route: /rerank, Methods: POST
INFO 06-03 21:18:16 [launcher.py:36] Route: /v1/rerank, Methods: POST
INFO 06-03 21:18:16 [launcher.py:36] Route: /v2/rerank, Methods: POST
INFO 06-03 21:18:16 [launcher.py:36] Route: /invocations, Methods: POST
INFO 06-03 21:18:16 [launcher.py:36] Route: /metrics, Methods: GET
INFO:     Started server process [1352699]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     127.0.0.1:46476 - "GET /v1/models HTTP/1.1" 200 OK
INFO 06-03 21:26:10 [logger.py:39] Received request cmpl-e5b43d85e5724334a550a095716997a7-0: prompt: 'What is MIT?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=600, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [8197, 438, 7131, 49], lora_request: None, prompt_adapter_request: None.
INFO 06-03 21:26:10 [logger.py:39] Received request cmpl-e5b43d85e5724334a550a095716997a7-1: prompt: '<|start_of_role|>user<|end_of_role|>What is MIT?<|end_of_text|>\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=600, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [49152, 496, 49153, 8197, 438, 7131, 49, 0, 203], lora_request: None, prompt_adapter_request: None.
INFO 06-03 21:26:10 [async_llm.py:255] Added request cmpl-e5b43d85e5724334a550a095716997a7-0.
INFO 06-03 21:26:10 [async_llm.py:255] Added request cmpl-e5b43d85e5724334a550a095716997a7-1.
INFO:     127.0.0.1:57108 - "POST /v1/completions HTTP/1.1" 200 OK
WARNING 06-03 21:26:13 [tokenizer.py:294] No tokenizer found in /proj/dmfexp/statllm/users/kgreenewald/.cache/huggingface/models/hub/models--ibm-granite--granite-3.2-8b-alora-uncertainty/snapshots/6109ad88201426003e696d023ec67c19e7f3d444, using base model tokenizer instead. (Exception: <class 'transformers.models.granite.configuration_granite.GraniteConfig'>)
INFO 06-03 21:26:13 [logger.py:39] Received request cmpl-ee24a5c42dc049708804710682853909-0: prompt: "What is MIT?\n\nMIT, or the Massachusetts Institute of Technology, is a prestigious private research university located in Cambridge, Massachusetts, United States. It is renowned for its strong emphasis on scientific, technological, and engineering education and research.\n\nMIT was founded in 1861 and is consistently ranked among the top universities globally. The institution is known for its rigorous academic programs, innovative research, and significant contributions to various fields, including computer science, engineering, physics, mathematics, and economics.\n\nMIT's campus is home to numerous research centers, laboratories, and institutes, which facilitate groundbreaking discoveries and advancements. The university is also recognized for its commitment to fostering a diverse and inclusive community, as well as its dedication to addressing global challenges through interdisciplinary collaboration.\n\nMIT's alumni and faculty have made significant impacts in various industries, including technology, academia, and government. Some notable alumni include physicist Richard Feynman, computer scientist Marvin Minsky, and entrepreneur Elon Musk.\n\nIn summary, MIT is a world-class research university with a strong focus on science, technology, and engineering. Its commitment to innovation, research, and education has solidified its reputation as a leading institution in the global academic landscape.<|end_of_text|>\n<|start_of_role|>certainty<|end_of_role|>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [8197, 438, 7131, 49, 203, 203, 6675, 30, 556, 322, 28051, 867, 43017, 1421, 27358, 432, 21909, 30, 438, 312, 41804, 365, 2406, 945, 13234, 707, 9190, 15282, 328, 31719, 12154, 30, 28051, 867, 43017, 1421, 30, 28882, 18600, 32, 2030, 438, 316, 2481, 318, 436, 2819, 12101, 12480, 22557, 544, 31722, 30, 7341, 1347, 11081, 30, 461, 33627, 31741, 461, 13234, 32, 203, 203, 6675, 1597, 16870, 6918, 328, 225, 35, 42, 40, 35, 461, 438, 45631, 11263, 318, 17660, 322, 2663, 707, 5810, 2105, 29585, 32, 886, 31681, 438, 8967, 436, 2819, 34315, 271, 3291, 312, 31505, 18335, 30, 328, 15007, 1353, 13234, 30, 461, 14724, 25198, 372, 10297, 3829, 30, 6237, 10670, 27536, 30, 33627, 30, 28231, 30, 32775, 101, 30, 461, 32848, 1316, 32, 203, 203, 6675, 1182, 16303, 352, 438, 6765, 372, 21639, 352, 13234, 40962, 30, 20173, 717, 732, 30, 461, 1440, 283, 3676, 30, 1510, 44288, 16398, 20806, 23149, 732, 461, 18514, 1728, 32, 886, 707, 9190, 438, 2329, 27149, 436, 2819, 4583, 469, 372, 296, 649, 8637, 312, 4451, 24026, 461, 35216, 11896, 30, 619, 4487, 619, 2819, 20405, 367, 372, 43383, 3649, 35439, 3919, 1426, 1285, 22257, 3172, 39973, 32, 203, 203, 6675, 1182, 743, 1129, 91, 461, 9652, 31146, 1159, 5590, 14724, 14932, 101, 328, 10297, 4842, 675, 3064, 30, 6237, 21519, 30, 312, 9002, 95, 905, 30, 461, 31789, 32, 4185, 646, 444, 743, 1129, 91, 2305, 26571, 295, 427, 39042, 15311, 841, 1588, 30, 10670, 2197, 1606, 427, 8359, 13762, 7507, 14265, 30, 461, 17817, 1001, 941, 305, 516, 9512, 488, 352, 93, 32, 203, 203, 383, 8822, 30, 7131, 438, 312, 5788, 31, 823, 13234, 707, 9190, 623, 312, 12101, 9280, 544, 27536, 30, 21519, 30, 461, 33627, 32, 26983, 4583, 469, 372, 328, 38795, 30, 13234, 30, 461, 31741, 1401, 4810, 1639, 2819, 316, 45783, 619, 312, 15996, 31681, 328, 322, 3649, 312, 31505, 37009, 32, 0, 203, 49152, 6989, 24933, 49153], lora_request: LoRARequest(lora_name='new_alora', lora_int_id=1, lora_path='/proj/dmfexp/statllm/users/kgreenewald/.cache/huggingface/models/hub/models--ibm-granite--granite-3.2-8b-alora-uncertainty/snapshots/6109ad88201426003e696d023ec67c19e7f3d444', lora_local_path=None, long_lora_max_len=None, base_model_name='ibm-granite/granite-3.2-8b-instruct'), prompt_adapter_request: None.
INFO 06-03 21:26:13 [logger.py:39] Received request cmpl-ee24a5c42dc049708804710682853909-1: prompt: "<|start_of_role|>user<|end_of_role|>What is MIT?<|end_of_text|>\n\nMIT, or the Massachusetts Institute of Technology, is a prestigious private research university located in Cambridge, Massachusetts, United States. Established in 1861, MIT is renowned for its commitment to advancing knowledge and education in science, technology, engineering, arts, and humanities. It is consistently ranked among the top universities globally for its academic rigor, research output, and innovation.\n\nMIT is known for its strong emphasis on undergraduate education, offering a wide range of disciplines and fostering a collaborative learning environment. The institute is home to numerous research centers and laboratories, where faculty and students work together on cutting-edge projects in various fields, such as artificial intelligence, robotics, materials science, and environmental sustainability.\n\nThe university's campus is a vibrant hub of innovation, with state-of-the-art facilities and resources dedicated to fostering creativity and intellectual curiosity. MIT's alumni include numerous influential figures in technology, science, and business, contributing to its reputation as a global leader in higher education and research.\n\nIf you have any specific questions about MIT, its programs, research, or notable achievements, feel free to ask! I'm here to help.<|end_of_text|>\n<|start_of_role|>certainty<|end_of_role|>", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [49152, 496, 49153, 8197, 438, 7131, 49, 0, 203, 203, 6675, 30, 556, 322, 28051, 867, 43017, 1421, 27358, 432, 21909, 30, 438, 312, 41804, 365, 2406, 945, 13234, 707, 9190, 15282, 328, 31719, 12154, 30, 28051, 867, 43017, 1421, 30, 28882, 18600, 32, 14164, 360, 5829, 328, 225, 35, 42, 40, 35, 30, 7131, 438, 316, 2481, 318, 436, 2819, 4583, 469, 372, 9767, 18522, 16969, 461, 31741, 328, 27536, 30, 21519, 30, 33627, 30, 5549, 101, 30, 461, 13462, 2105, 32, 2030, 438, 45631, 11263, 318, 17660, 322, 2663, 707, 5810, 2105, 29585, 436, 2819, 312, 31505, 34315, 271, 30, 13234, 1688, 30, 461, 328, 38795, 32, 203, 203, 6675, 438, 8967, 436, 2819, 12101, 12480, 22557, 544, 1531, 4943, 27415, 31741, 30, 45525, 312, 16264, 2155, 432, 1214, 22257, 2353, 461, 296, 649, 8637, 312, 21368, 1353, 9608, 4706, 32, 886, 1440, 11461, 438, 6765, 372, 21639, 352, 13234, 40962, 461, 20173, 717, 732, 30, 2154, 9652, 31146, 461, 16512, 1389, 10752, 544, 11909, 1054, 31, 5854, 8528, 328, 10297, 3829, 30, 3751, 619, 5549, 31251, 629, 21488, 30, 13047, 1316, 30, 12953, 27536, 30, 461, 4706, 279, 309, 35170, 3452, 32, 203, 203, 1318, 707, 9190, 1182, 16303, 352, 438, 312, 8943, 839, 692, 20089, 432, 328, 38795, 30, 623, 1603, 31, 1028, 31, 1382, 31, 502, 43798, 461, 6086, 23112, 372, 296, 649, 8637, 2020, 43826, 461, 31067, 43119, 2738, 3945, 543, 32, 7131, 1182, 743, 1129, 91, 2305, 21639, 352, 43229, 2722, 37526, 328, 21519, 30, 27536, 30, 461, 12590, 30, 23476, 372, 2819, 316, 45783, 619, 312, 3649, 22021, 328, 12224, 31741, 461, 13234, 32, 203, 203, 2797, 844, 1159, 1346, 2818, 10017, 2625, 7131, 30, 2819, 18335, 30, 13234, 30, 556, 646, 444, 14002, 42695, 30, 10871, 3741, 372, 7660, 19, 439, 3464, 2442, 372, 3049, 32, 0, 203, 49152, 6989, 24933, 49153], lora_request: LoRARequest(lora_name='new_alora', lora_int_id=1, lora_path='/proj/dmfexp/statllm/users/kgreenewald/.cache/huggingface/models/hub/models--ibm-granite--granite-3.2-8b-alora-uncertainty/snapshots/6109ad88201426003e696d023ec67c19e7f3d444', lora_local_path=None, long_lora_max_len=None, base_model_name='ibm-granite/granite-3.2-8b-instruct'), prompt_adapter_request: None.
INFO 06-03 21:26:13 [async_llm.py:255] Added request cmpl-ee24a5c42dc049708804710682853909-0.
INFO 06-03 21:26:13 [async_llm.py:255] Added request cmpl-ee24a5c42dc049708804710682853909-1.
INFO:     127.0.0.1:57108 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 06-03 21:26:16 [loggers.py:116] Engine 000: Avg prompt throughput: 65.0 tokens/s, Avg generation throughput: 63.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
INFO 06-03 21:26:26 [loggers.py:116] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
