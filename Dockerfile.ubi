## Global Args #################################################################
ARG BASE_UBI_IMAGE_TAG=9.3-1612
ARG PYTHON_VERSION=3.11
ARG PYTORCH_INDEX="https://download.pytorch.org/whl"
# ARG PYTORCH_INDEX="https://download.pytorch.org/whl/nightly"
ARG PYTORCH_VERSION=2.1.2

# NOTE: This setting only has an effect when not using prebuilt-wheel kernels
ARG TORCH_CUDA_ARCH_LIST="7.0 7.5 8.0 8.6 8.9 9.0+PTX"


## Base Layer ##################################################################
FROM registry.access.redhat.com/ubi9/ubi-minimal:${BASE_UBI_IMAGE_TAG} as base

RUN microdnf install -y \
    python3.11-pip python3.11-wheel \
    && microdnf clean all

WORKDIR /workspace

ENV LANG=C.UTF-8 \
    LC_ALL=C.UTF-8

# Some utils for dev purposes - tar required for kubectl cp
RUN microdnf install -y \
        which procps findutils tar vim \
    && microdnf clean all


## Python Installer ############################################################
FROM base as python-base

ARG PYTHON_VERSION

ENV VIRTUAL_ENV=/opt/vllm
ENV PATH="$VIRTUAL_ENV/bin:$PATH"
RUN microdnf install -y \
    python3.11-devel python3.11-pip python3.11-wheel && \
    python3.11 -m venv $VIRTUAL_ENV && microdnf clean all


## CUDA Base ###################################################################
FROM base as cuda-base

# The Nvidia operator won't allow deploying on CUDA 12.0 hosts if
# this env var is set to 12.2.0, even though it's compatible
#ENV CUDA_VERSION=12.2.0 \
ENV CUDA_VERSION=12.0.0 \
    NV_CUDA_LIB_VERSION=12.2.0-1 \
    NVIDIA_VISIBLE_DEVICES=all \
    NVIDIA_DRIVER_CAPABILITIES=compute,utility \
    NV_CUDA_CUDART_VERSION=12.2.53-1 \
    NV_CUDA_COMPAT_VERSION=535.104.12

RUN curl -Lo /etc/yum.repos.d/cuda-rhel9.repo \
        https://developer.download.nvidia.com/compute/cuda/repos/rhel9/x86_64/cuda-rhel9.repo

RUN microdnf install -y \
        cuda-cudart-12-2-${NV_CUDA_CUDART_VERSION} \
        cuda-compat-12-2-${NV_CUDA_COMPAT_VERSION} \
    && microdnf clean all


ARG CUDA_HOME="/usr/local/cuda"
ENV CUDA_HOME=${CUDA_HOME}\
    PATH="${CUDA_HOME}/bin:${PATH}" \
    LD_LIBRARY_PATH="${CUDA_HOME}/lib64:${CUDA_HOME}/extras/CUPTI/lib64:${LD_LIBRARY_PATH}"


## CUDA Development ############################################################
FROM cuda-base as cuda-devel

ENV NV_CUDA_CUDART_DEV_VERSION=12.2.53-1 \
    NV_NVML_DEV_VERSION=12.2.81-1 \
    NV_LIBCUBLAS_DEV_VERSION=12.2.1.16-1 \
    NV_LIBNPP_DEV_VERSION=12.1.1.14-1 \
    NV_LIBNCCL_DEV_PACKAGE_VERSION=2.18.5-1+cuda12.2

RUN microdnf install -y \
        cuda-command-line-tools-12-2-${NV_CUDA_LIB_VERSION} \
        cuda-libraries-devel-12-2-${NV_CUDA_LIB_VERSION} \
        cuda-minimal-build-12-2-${NV_CUDA_LIB_VERSION} \
        cuda-cudart-devel-12-2-${NV_CUDA_CUDART_DEV_VERSION} \
        cuda-nvml-devel-12-2-${NV_NVML_DEV_VERSION} \
        libcublas-devel-12-2-${NV_LIBCUBLAS_DEV_VERSION} \
        libnpp-devel-12-2-${NV_LIBNPP_DEV_VERSION} \
        libnccl-devel-${NV_LIBNCCL_DEV_PACKAGE_VERSION} \
    && microdnf clean all

ENV LIBRARY_PATH="$CUDA_HOME/lib64/stubs"

# Workaround for https://github.com/openai/triton/issues/2507 and
# https://github.com/pytorch/pytorch/issues/107960 -- hopefully
# this won't be needed for future versions of this docker image
# or future versions of triton.
RUN ldconfig /usr/local/cuda-12.2/compat/

## Development #################################################################
FROM cuda-devel AS build

ENV VIRTUAL_ENV=/opt/vllm
ENV PATH="$VIRTUAL_ENV/bin:$PATH"

RUN microdnf install -y python3.11-devel  \
    && microdnf clean all
COPY --from=python-base /opt/vllm /opt/vllm

# install compiler cache to speed up compilation leveraging local or remote caching
RUN rpm -ivh https://dl.fedoraproject.org/pub/epel/epel-release-latest-9.noarch.rpm && rpm -ql epel-release && microdnf install -y ccache && microdnf clean all

# files and directories related to build wheels
COPY csrc csrc
COPY setup.py setup.py
COPY cmake cmake
COPY CMakeLists.txt CMakeLists.txt
COPY Makefile Makefile
COPY proto proto
COPY requirements-build.txt requirements-build.txt
COPY requirements-common.txt requirements-common.txt
COPY requirements-cuda.txt requirements-cuda.txt
COPY pyproject.toml pyproject.toml
COPY vllm vllm

ARG TORCH_CUDA_ARCH_LIST
ENV TORCH_CUDA_ARCH_LIST=$TORCH_CUDA_ARCH_LIST

# max jobs used by Ninja to build extensions
ARG max_jobs=2
ENV MAX_JOBS=${max_jobs}
# number of threads used by nvcc
ARG nvcc_threads=8
ENV NVCC_THREADS=$nvcc_threads
# make sure punica kernels are built (for LoRA)
ENV VLLM_INSTALL_PUNICA_KERNELS=1

# Make sure the cuda environment is in the PATH
ENV PATH=/usr/local/cuda/bin:$PATH
ENV LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH

ENV CCACHE_DIR=/root/.cache/ccache

RUN --mount=type=cache,target=/root/.cache/ccache \
    --mount=type=cache,target=/root/.cache/pip \
    pip install -r requirements-build.txt && \
    make gen-protos && \
    python3.11 setup.py bdist_wheel --dist-dir=dist

# the `vllm_nccl` package must be installed from source distribution
# pip is too smart to store a wheel in the cache, and other CI jobs
# will directly use the wheel from the cache, which is not what we want.
# we need to remove it manually
RUN --mount=type=cache,target=/root/.cache/pip \
    pip cache remove vllm_nccl*
#################### EXTENSION Build IMAGE ####################

#################### FLASH_ATTENTION Build IMAGE ####################
FROM build as flash-attn-builder
ENV VIRTUAL_ENV=/opt/vllm/bin
ENV PATH=${VIRTUAL_ENV}/bin:$PATH

RUN microdnf install -y git \
    && microdnf clean all

# max jobs used for build
ARG max_jobs=2
ENV MAX_JOBS=${max_jobs}
# flash attention version
ARG flash_attn_version=v2.5.6
ENV FLASH_ATTN_VERSION=${flash_attn_version}

WORKDIR /usr/src/flash-attention-v2

# Download the wheel or build it if a pre-compiled release doesn't exist
RUN --mount=type=cache,target=/root/.cache/pip \
    pip install wheel && \
    pip --verbose wheel flash-attn==${FLASH_ATTN_VERSION} \
    --no-build-isolation --no-deps

## Release #####################################################################
FROM base AS vllm-openai

WORKDIR /workspace

# Create release python environment
COPY --from=python-base /opt/vllm /opt/vllm
ENV VIRTUAL_ENV=/opt/vllm
ENV PATH=$VIRTUAL_ENV/bin/:$PATH

# Triton needs a CC compiler
RUN microdnf install -y gcc \
    && microdnf clean all

RUN --mount=type=cache,target=/root/.cache/pip \
    pip install \
        # additional dependencies for the TGIS gRPC server
        grpcio==1.62.1 \
        # additional dependencies for openai api_server
        accelerate==0.28.0 \
        # hf_transfer for faster HF hub downloads
        hf_transfer==0.1.6

# Install flash attention (from pre-built wheel)
RUN --mount=type=bind,from=flash-attn-builder,src=/usr/src/flash-attention-v2,target=/usr/src/flash-attention-v2 \
    pip install /usr/src/flash-attention-v2/*.whl --no-cache-dir

# vLLM will not be installed in site-packages
COPY --from=build /workspace/dist/*whl /tmp
RUN --mount=type=cache,target=/root/.cache/pip \
    pip install /tmp/*whl && rm -f /tmp/*whl

ENV HF_HUB_OFFLINE=1 \
    PORT=8000 \
    GRPC_PORT=8033 \
    HOME=/home/vllm \
    VLLM_NCCL_SO_PATH=/opt/vllm/lib/python3.11/site-packages/nvidia/nccl/lib/libnccl.so.2 \
    VLLM_USAGE_SOURCE=production-docker-image

# setup non-root user for OpenShift
RUN microdnf install -y shadow-utils \
    && umask 002 \
    && useradd --uid 2000 --gid 0 vllm \
    && microdnf remove -y shadow-utils \
    && microdnf clean all \
    && chmod g+rwx $HOME /usr/src /workspace

COPY LICENSE /licenses/vllm.md

USER 2000
ENTRYPOINT ["python3", "-m", "vllm.entrypoints.openai.api_server"]